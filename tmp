Name:               ubuntu
Namespace:          kube-ddl
Priority:           0
PriorityClassName:  <none>
Node:               se004/10.1.2.4
Start Time:         Fri, 26 Apr 2019 12:41:40 +0000
Labels:             <none>
Annotations:        kubectl.kubernetes.io/last-applied-configuration:
                      {"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{},"name":"ubuntu","namespace":"kube-ddl"},"spec":{"containers":[{"command":["/b...
Status:             Running
IP:                 10.244.90.3
Containers:
  ubuntu:
    Container ID:  docker://066a936b77fda9d06699ab47e7776762931ea1f4e33a3287b72af3fc138fec01
    Image:         xinyao1994/ubuntu-ddl:v.1.0.0
    Image ID:      docker-pullable://xinyao1994/ubuntu-ddl@sha256:f2557f94cac1cc4509d0483cb6e302da841ecd6f82eb2e91dc7ba6cfd0c580ab
    Port:          22/TCP
    Host Port:     0/TCP
    Command:
      /bin/bash
      -ce
      tail -f /dev/null
    State:          Running
      Started:      Fri, 26 Apr 2019 12:41:53 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-nggjd (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  default-token-nggjd:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-nggjd
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:               coredns-fb8b8dccf-h8vdx
Namespace:          kube-system
Priority:           2000000000
PriorityClassName:  system-cluster-critical
Node:               se045/10.1.2.45
Start Time:         Fri, 26 Apr 2019 04:42:00 +0000
Labels:             k8s-app=kube-dns
                    pod-template-hash=fb8b8dccf
Annotations:        <none>
Status:             Running
IP:                 10.244.131.2
Controlled By:      ReplicaSet/coredns-fb8b8dccf
Containers:
  coredns:
    Container ID:  docker://9b73b5608bf740715f779f502247574759073057b562a39a5141e9b0f889a96e
    Image:         k8s.gcr.io/coredns:1.3.1
    Image ID:      docker-pullable://k8s.gcr.io/coredns@sha256:02382353821b12c21b062c59184e227e001079bb13ebd01f9d3270ba0fcbf1e4
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Fri, 26 Apr 2019 04:42:06 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8080/health delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from coredns-token-84zjh (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  coredns-token-84zjh:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  coredns-token-84zjh
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  beta.kubernetes.io/os=linux
Tolerations:     CriticalAddonsOnly
                 node-role.kubernetes.io/master:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:               coredns-fb8b8dccf-w4xmk
Namespace:          kube-system
Priority:           2000000000
PriorityClassName:  system-cluster-critical
Node:               se047/10.1.2.47
Start Time:         Fri, 26 Apr 2019 04:42:14 +0000
Labels:             k8s-app=kube-dns
                    pod-template-hash=fb8b8dccf
Annotations:        <none>
Status:             Running
IP:                 10.244.133.2
Controlled By:      ReplicaSet/coredns-fb8b8dccf
Containers:
  coredns:
    Container ID:  docker://56042d18976a3a363d7bd77c35a561989b6ee07cf0492a68d21060bc5d9c001f
    Image:         k8s.gcr.io/coredns:1.3.1
    Image ID:      docker-pullable://k8s.gcr.io/coredns@sha256:02382353821b12c21b062c59184e227e001079bb13ebd01f9d3270ba0fcbf1e4
    Ports:         53/UDP, 53/TCP, 9153/TCP
    Host Ports:    0/UDP, 0/TCP, 0/TCP
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Fri, 26 Apr 2019 04:42:20 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8080/health delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from coredns-token-84zjh (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  coredns-token-84zjh:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  coredns-token-84zjh
    Optional:    false
QoS Class:       Burstable
Node-Selectors:  beta.kubernetes.io/os=linux
Tolerations:     CriticalAddonsOnly
                 node-role.kubernetes.io/master:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:               etcd-se064
Namespace:          kube-system
Priority:           2000000000
PriorityClassName:  system-cluster-critical
Node:               se064/10.1.2.64
Start Time:         Thu, 25 Apr 2019 12:45:48 +0000
Labels:             component=etcd
                    tier=control-plane
Annotations:        kubernetes.io/config.hash: b38ce05694a773ba5c9779a3e078ce12
                    kubernetes.io/config.mirror: b38ce05694a773ba5c9779a3e078ce12
                    kubernetes.io/config.seen: 2019-04-25T12:45:47.648700298Z
                    kubernetes.io/config.source: file
Status:             Running
IP:                 10.1.2.64
Containers:
  etcd:
    Container ID:  docker://0baf02c55eae2b0d2aaca31c274ca9bddd110b38d770053b9c5f7ecf94a73df3
    Image:         k8s.gcr.io/etcd:3.3.10
    Image ID:      docker-pullable://k8s.gcr.io/etcd@sha256:17da501f5d2a675be46040422a27b7cc21b8a43895ac998b171db1c346f361f7
    Port:          <none>
    Host Port:     <none>
    Command:
      etcd
      --advertise-client-urls=https://10.1.2.64:2379
      --cert-file=/etc/kubernetes/pki/etcd/server.crt
      --client-cert-auth=true
      --data-dir=/var/lib/etcd
      --initial-advertise-peer-urls=https://10.1.2.64:2380
      --initial-cluster=se064=https://10.1.2.64:2380
      --key-file=/etc/kubernetes/pki/etcd/server.key
      --listen-client-urls=https://127.0.0.1:2379,https://10.1.2.64:2379
      --listen-peer-urls=https://10.1.2.64:2380
      --name=se064
      --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      --peer-client-cert-auth=true
      --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      --snapshot-count=10000
      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
    State:          Running
      Started:      Thu, 25 Apr 2019 12:45:50 +0000
    Ready:          True
    Restart Count:  0
    Liveness:       exec [/bin/sh -ec ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt --key=/etc/kubernetes/pki/etcd/healthcheck-client.key get foo] delay=15s timeout=15s period=10s #success=1 #failure=8
    Environment:    <none>
    Mounts:
      /etc/kubernetes/pki/etcd from etcd-certs (rw)
      /var/lib/etcd from etcd-data (rw)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  etcd-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki/etcd
    HostPathType:  DirectoryOrCreate
  etcd-data:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/etcd
    HostPathType:  DirectoryOrCreate
QoS Class:         BestEffort
Node-Selectors:    <none>
Tolerations:       :NoExecute
Events:            <none>


Name:               heapster-88c8457bc-w6zth
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se051/10.1.2.51
Start Time:         Fri, 26 Apr 2019 04:51:00 +0000
Labels:             k8s-app=heapster
                    pod-template-hash=88c8457bc
                    task=monitoring
Annotations:        <none>
Status:             Running
IP:                 10.244.137.2
Controlled By:      ReplicaSet/heapster-88c8457bc
Containers:
  heapster:
    Container ID:  docker://9e963523b6853d8d90c0979d386e75855069c3e7c47b29c049dac74a2859aee2
    Image:         k8s.gcr.io/heapster-amd64:v1.5.4
    Image ID:      docker-pullable://k8s.gcr.io/heapster-amd64@sha256:dccaabb0c20cf05c29baefa1e9bf0358b083ccc0fab492b9b3b47fb7e4db5472
    Port:          <none>
    Host Port:     <none>
    Command:
      /heapster
      --source=kubernetes.summary_api:https://kubernetes.default?kubeletHttps=true&kubeletPort=10250&insecure=true&useServiceAccount=true
      --sink=influxdb:http://monitoring-influxdb.kube-system.svc:8086
    State:          Running
      Started:      Fri, 26 Apr 2019 04:51:07 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from heapster-token-5nbq4 (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  heapster-token-5nbq4:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  heapster-token-5nbq4
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:               kube-apiserver-se064
Namespace:          kube-system
Priority:           2000000000
PriorityClassName:  system-cluster-critical
Node:               se064/10.1.2.64
Start Time:         Thu, 25 Apr 2019 12:45:47 +0000
Labels:             component=kube-apiserver
                    tier=control-plane
Annotations:        kubernetes.io/config.hash: dc80c5cc465030725e0be59956764caf
                    kubernetes.io/config.mirror: dc80c5cc465030725e0be59956764caf
                    kubernetes.io/config.seen: 2019-04-25T12:45:47.648686089Z
                    kubernetes.io/config.source: file
Status:             Running
IP:                 10.1.2.64
Containers:
  kube-apiserver:
    Container ID:  docker://00344c45d634bddb865a098dbeae9afab9f1fdb5e599ef5c7eb8aeaabda37b30
    Image:         k8s.gcr.io/kube-apiserver:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-apiserver@sha256:bb3e3264bf74cc6929ec05b494d95b7aed9ee1e5c1a5c8e0693b0f89e2e7288e
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-apiserver
      --advertise-address=10.1.2.64
      --allow-privileged=true
      --authorization-mode=Node,RBAC
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --enable-admission-plugins=NodeRestriction
      --enable-bootstrap-token-auth=true
      --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      --etcd-servers=https://127.0.0.1:2379
      --insecure-port=0
      --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      --requestheader-allowed-names=front-proxy-client
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --requestheader-extra-headers-prefix=X-Remote-Extra-
      --requestheader-group-headers=X-Remote-Group
      --requestheader-username-headers=X-Remote-User
      --secure-port=6443
      --service-account-key-file=/etc/kubernetes/pki/sa.pub
      --service-cluster-ip-range=10.244.0.0/12
      --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    State:          Running
      Started:      Thu, 25 Apr 2019 12:45:49 +0000
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        250m
    Liveness:     http-get https://10.1.2.64:6443/healthz delay=15s timeout=15s period=10s #success=1 #failure=8
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute
Events:            <none>


Name:               kube-controller-manager-se064
Namespace:          kube-system
Priority:           2000000000
PriorityClassName:  system-cluster-critical
Node:               se064/10.1.2.64
Start Time:         Thu, 25 Apr 2019 12:45:48 +0000
Labels:             component=kube-controller-manager
                    tier=control-plane
Annotations:        kubernetes.io/config.hash: f423ac50e24b65e6d66fe37e6d721912
                    kubernetes.io/config.mirror: f423ac50e24b65e6d66fe37e6d721912
                    kubernetes.io/config.seen: 2019-04-25T12:45:47.648695558Z
                    kubernetes.io/config.source: file
Status:             Running
IP:                 10.1.2.64
Containers:
  kube-controller-manager:
    Container ID:  docker://a50423d39dc4154061c2188e21024b5f986c8f1d3fe7991b037adcecf8678812
    Image:         k8s.gcr.io/kube-controller-manager:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-controller-manager@sha256:5279e0030094c0ef2ba183bd9627e91e74987477218396bd97a5e070df241df5
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-controller-manager
      --allocate-node-cidrs=true
      --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      --bind-address=127.0.0.1
      --client-ca-file=/etc/kubernetes/pki/ca.crt
      --cluster-cidr=10.244.0.0/16
      --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      --controllers=*,bootstrapsigner,tokencleaner
      --kubeconfig=/etc/kubernetes/controller-manager.conf
      --leader-elect=true
      --node-cidr-mask-size=24
      --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      --root-ca-file=/etc/kubernetes/pki/ca.crt
      --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      --use-service-account-credentials=true
    State:          Running
      Started:      Thu, 25 Apr 2019 12:45:50 +0000
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        200m
    Liveness:     http-get http://127.0.0.1:10252/healthz delay=15s timeout=15s period=10s #success=1 #failure=8
    Environment:  <none>
    Mounts:
      /etc/ca-certificates from etc-ca-certificates (ro)
      /etc/kubernetes/controller-manager.conf from kubeconfig (ro)
      /etc/kubernetes/pki from k8s-certs (ro)
      /etc/ssl/certs from ca-certs (ro)
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir (rw)
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates (ro)
      /usr/share/ca-certificates from usr-share-ca-certificates (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute
Events:            <none>


Name:               kube-flannel-ds-amd64-294ss
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se053/10.1.2.53
Start Time:         Sat, 27 Apr 2019 03:48:25 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.53
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://5641b69a257c8df929f3f226b81ff983ec408ad8296e97e0e24ddc291580bdad
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:27 +0000
      Finished:     Sat, 27 Apr 2019 03:48:27 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://52e333e24b3f6fce6a1f3fe8d3b0219674c7e728cc3008b27440c67ae0b910bf
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:28 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-294ss (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  5m18s  default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-294ss to se053
  Normal  Pulled     5m17s  kubelet, se053     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m16s  kubelet, se053     Created container install-cni
  Normal  Started    5m16s  kubelet, se053     Started container install-cni
  Normal  Pulled     5m15s  kubelet, se053     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m15s  kubelet, se053     Created container kube-flannel
  Normal  Started    5m15s  kubelet, se053     Started container kube-flannel


Name:               kube-flannel-ds-amd64-2g66q
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se009/10.1.2.9
Start Time:         Sat, 27 Apr 2019 03:48:24 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.9
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://db1c534a5d4ba04465e242514f6a7985f862a8af3c0fb7bbef69740cbd901381
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:26 +0000
      Finished:     Sat, 27 Apr 2019 03:48:26 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://49bef365619dd2c744b7b4872cd7e8c8dd1ed0e2c86875beaf7ff5f6448144f8
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:28 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-2g66q (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type     Reason            Age                  From               Message
  ----     ------            ----                 ----               -------
  Normal   Scheduled         5m19s                default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-2g66q to se009
  Normal   Pulled            5m17s                kubelet, se009     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal   Created           5m17s                kubelet, se009     Created container install-cni
  Normal   Started           5m17s                kubelet, se009     Started container install-cni
  Normal   Pulled            5m16s                kubelet, se009     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal   Created           5m16s                kubelet, se009     Created container kube-flannel
  Normal   Started           5m15s                kubelet, se009     Started container kube-flannel
  Warning  DNSConfigForming  20s (x9 over 5m18s)  kubelet, se009     Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 147.8.176.15 147.8.178.15 147.8.175.12


Name:               kube-flannel-ds-amd64-2kd2g
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se048/10.1.2.48
Start Time:         Sat, 27 Apr 2019 03:48:24 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.48
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://c476bb965822e86d67fe0b548f4b5ac0d4522bdf649240d2724b0147cf227e17
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:25 +0000
      Finished:     Sat, 27 Apr 2019 03:48:25 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://189b6eee1dc1e3506a2f7e67529760a22946acfa1b01fbe147267395e1e59b61
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:26 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-2kd2g (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  5m20s  default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-2kd2g to se048
  Normal  Pulled     5m18s  kubelet, se048     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m18s  kubelet, se048     Created container install-cni
  Normal  Started    5m18s  kubelet, se048     Started container install-cni
  Normal  Pulled     5m17s  kubelet, se048     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m17s  kubelet, se048     Created container kube-flannel
  Normal  Started    5m17s  kubelet, se048     Started container kube-flannel


Name:               kube-flannel-ds-amd64-4l2zb
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se060/10.1.2.60
Start Time:         Sat, 27 Apr 2019 03:48:23 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.60
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://bbd533d799e7a50d2c08f8b018bd799f730529a1dc9005831ba9d546ae46f796
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:25 +0000
      Finished:     Sat, 27 Apr 2019 03:48:25 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://eb33dcabae2355d0cf4649f2710eb7a3249c6a4f8be8451b2b46b09d22f1dc70
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:26 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-4l2zb (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  5m20s  default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-4l2zb to se060
  Normal  Pulled     5m19s  kubelet, se060     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m18s  kubelet, se060     Created container install-cni
  Normal  Started    5m18s  kubelet, se060     Started container install-cni
  Normal  Pulled     5m17s  kubelet, se060     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m17s  kubelet, se060     Created container kube-flannel
  Normal  Started    5m17s  kubelet, se060     Started container kube-flannel


Name:               kube-flannel-ds-amd64-4v7ql
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se045/10.1.2.45
Start Time:         Sat, 27 Apr 2019 03:48:25 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.45
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://f978c65341da63946f52781d05f0a6088ebe711ad9b841dc28296e0f77693153
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:27 +0000
      Finished:     Sat, 27 Apr 2019 03:48:27 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://ef2143a68850f65aa460910b14b7a8bc8c298837758f763b454588d440b84b3c
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:28 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-4v7ql (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  5m18s  default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-4v7ql to se045
  Normal  Pulled     5m17s  kubelet, se045     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m16s  kubelet, se045     Created container install-cni
  Normal  Started    5m16s  kubelet, se045     Started container install-cni
  Normal  Pulled     5m16s  kubelet, se045     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m15s  kubelet, se045     Created container kube-flannel
  Normal  Started    5m15s  kubelet, se045     Started container kube-flannel


Name:               kube-flannel-ds-amd64-56wmw
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se004/10.1.2.4
Start Time:         Sat, 27 Apr 2019 03:48:24 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.4
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://61557fedd5dc2447b81b75cece26c5b2c8d38d746e7b48b63e156079bfb5aba2
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:25 +0000
      Finished:     Sat, 27 Apr 2019 03:48:25 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://42f937342a1912b6aa147e56b7746daca4e500afb3b08922472807558d2e17bc
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:27 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-56wmw (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  5m19s  default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-56wmw to se004
  Normal  Pulled     5m18s  kubelet, se004     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m18s  kubelet, se004     Created container install-cni
  Normal  Started    5m18s  kubelet, se004     Started container install-cni
  Normal  Pulled     5m17s  kubelet, se004     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m17s  kubelet, se004     Created container kube-flannel
  Normal  Started    5m16s  kubelet, se004     Started container kube-flannel


Name:               kube-flannel-ds-amd64-59497
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se014/10.1.2.14
Start Time:         Sat, 27 Apr 2019 03:48:25 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.14
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://fb8db83b495c3c90397b9d2981263cf23e126f29002054098a2de8c6d6794c77
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:26 +0000
      Finished:     Sat, 27 Apr 2019 03:48:26 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://8d9d7a3742c03d0e47dcaa7ebbbf12f40e28c74da3f837e5c221e7338df82881
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:27 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-59497 (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type     Reason            Age                  From               Message
  ----     ------            ----                 ----               -------
  Normal   Scheduled         5m18s                default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-59497 to se014
  Normal   Pulled            5m17s                kubelet, se014     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal   Created           5m17s                kubelet, se014     Created container install-cni
  Normal   Started           5m17s                kubelet, se014     Started container install-cni
  Normal   Started           5m16s                kubelet, se014     Started container kube-flannel
  Normal   Pulled            5m16s                kubelet, se014     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal   Created           5m16s                kubelet, se014     Created container kube-flannel
  Warning  DNSConfigForming  81s (x8 over 5m18s)  kubelet, se014     Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 147.8.176.15 147.8.178.15 147.8.175.12


Name:               kube-flannel-ds-amd64-5wd9k
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se042/10.1.2.42
Start Time:         Sat, 27 Apr 2019 03:48:25 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.42
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://c659c7587f2c283f1d1796ac3d1d980e82a41cc6d764333bdd00df76877bae0c
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:27 +0000
      Finished:     Sat, 27 Apr 2019 03:48:27 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://3e9fd60640e92881412b2f24de574cf2e48557692d9f5af3acdafbb4a2e064ed
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:28 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-5wd9k (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  5m18s  default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-5wd9k to se042
  Normal  Pulled     5m17s  kubelet, se042     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m16s  kubelet, se042     Created container install-cni
  Normal  Started    5m16s  kubelet, se042     Started container install-cni
  Normal  Pulled     5m16s  kubelet, se042     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m15s  kubelet, se042     Created container kube-flannel
  Normal  Started    5m15s  kubelet, se042     Started container kube-flannel


Name:               kube-flannel-ds-amd64-62kkf
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se020/10.1.2.20
Start Time:         Sat, 27 Apr 2019 03:48:24 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.20
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://e5ca8fb961d93559491831636ded514e0df93496944273fcf0500982bf31163c
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:26 +0000
      Finished:     Sat, 27 Apr 2019 03:48:26 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://0b31cd3517bb2831c2019053b5a6d143c0164989323238539b86ac05b9da7af7
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:27 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-62kkf (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type     Reason            Age                 From               Message
  ----     ------            ----                ----               -------
  Normal   Scheduled         5m19s               default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-62kkf to se020
  Normal   Pulled            5m18s               kubelet, se020     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal   Created           5m17s               kubelet, se020     Created container install-cni
  Normal   Started           5m17s               kubelet, se020     Started container install-cni
  Normal   Pulled            5m17s               kubelet, se020     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal   Started           5m16s               kubelet, se020     Started container kube-flannel
  Normal   Created           5m16s               kubelet, se020     Created container kube-flannel
  Warning  DNSConfigForming  6s (x9 over 5m18s)  kubelet, se020     Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 147.8.176.15 147.8.178.15 147.8.175.12


Name:               kube-flannel-ds-amd64-6t7kg
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se040/10.1.2.40
Start Time:         Sat, 27 Apr 2019 03:48:25 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.40
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://1ec16800b62548b5c31a810ec0b26ebfb09350845fefdcff64be41050994986f
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:26 +0000
      Finished:     Sat, 27 Apr 2019 03:48:26 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://c51eb63009e26b194e4129c6f1f9817c711c9ca0740f1c38e83016a30a3dafa6
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:28 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-6t7kg (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  5m18s  default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-6t7kg to se040
  Normal  Pulled     5m17s  kubelet, se040     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m17s  kubelet, se040     Created container install-cni
  Normal  Started    5m17s  kubelet, se040     Started container install-cni
  Normal  Pulled     5m16s  kubelet, se040     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m15s  kubelet, se040     Created container kube-flannel
  Normal  Started    5m15s  kubelet, se040     Started container kube-flannel


Name:               kube-flannel-ds-amd64-6trpp
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se032/10.1.2.32
Start Time:         Sat, 27 Apr 2019 03:48:24 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.32
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://fbb7dc6deb0613113759f8c9e7fbb88a7dd4957959c05b758ad686ca93ce3495
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:26 +0000
      Finished:     Sat, 27 Apr 2019 03:48:26 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://0e51997749b981e18042cb817568424eb46fbbcf41ded5027c93dac42a68f60c
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:27 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-6trpp (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  5m19s  default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-6trpp to se032
  Normal  Pulled     5m18s  kubelet, se032     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m18s  kubelet, se032     Created container install-cni
  Normal  Started    5m17s  kubelet, se032     Started container install-cni
  Normal  Pulled     5m17s  kubelet, se032     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m17s  kubelet, se032     Created container kube-flannel
  Normal  Started    5m16s  kubelet, se032     Started container kube-flannel


Name:               kube-flannel-ds-amd64-6vd4w
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se008/10.1.2.8
Start Time:         Sat, 27 Apr 2019 03:48:24 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.8
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://01b3f2af1d5e3d8129217bce69c3623ae58b36fdfb2e9b8541bd46de8f2649ea
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:26 +0000
      Finished:     Sat, 27 Apr 2019 03:48:26 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://745a764511f3c1602ddae4d7eab296168f48c4996db25b74ca5c5feedca31f45
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:27 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-6vd4w (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type     Reason            Age                  From               Message
  ----     ------            ----                 ----               -------
  Normal   Scheduled         5m20s                default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-6vd4w to se008
  Normal   Pulled            5m18s                kubelet, se008     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal   Created           5m17s                kubelet, se008     Created container install-cni
  Normal   Started           5m17s                kubelet, se008     Started container install-cni
  Normal   Started           5m16s                kubelet, se008     Started container kube-flannel
  Normal   Pulled            5m16s                kubelet, se008     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal   Created           5m16s                kubelet, se008     Created container kube-flannel
  Warning  DNSConfigForming  29s (x9 over 5m18s)  kubelet, se008     Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 147.8.176.15 147.8.178.15 147.8.175.12


Name:               kube-flannel-ds-amd64-946ls
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se044/10.1.2.44
Start Time:         Sat, 27 Apr 2019 03:48:24 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.44
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://b92d359222fef59dbcf424d012ace9a149b733b8807b852b86334300c3598556
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:26 +0000
      Finished:     Sat, 27 Apr 2019 03:48:26 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://c39fc3aba1d7d1e30481f3ad7d28d337aecd24f0d1b88cc6ae2084cb72332dac
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:27 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-946ls (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  5m19s  default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-946ls to se044
  Normal  Pulled     5m18s  kubelet, se044     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m18s  kubelet, se044     Created container install-cni
  Normal  Started    5m17s  kubelet, se044     Started container install-cni
  Normal  Pulled     5m17s  kubelet, se044     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m17s  kubelet, se044     Created container kube-flannel
  Normal  Started    5m16s  kubelet, se044     Started container kube-flannel


Name:               kube-flannel-ds-amd64-9ncbd
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se031/10.1.2.31
Start Time:         Sat, 27 Apr 2019 03:48:23 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.31
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://2622ebc490c51d9a8563424824a8391bcac2d8101e004f59b0083e8537e54600
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:25 +0000
      Finished:     Sat, 27 Apr 2019 03:48:25 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://5f59ebaebc9ed1dbd0875f34c6790ab621048c40a23a104eb20c42455a9f913c
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:26 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-9ncbd (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  5m20s  default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-9ncbd to se031
  Normal  Pulled     5m19s  kubelet, se031     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m18s  kubelet, se031     Created container install-cni
  Normal  Started    5m18s  kubelet, se031     Started container install-cni
  Normal  Pulled     5m18s  kubelet, se031     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m17s  kubelet, se031     Created container kube-flannel
  Normal  Started    5m17s  kubelet, se031     Started container kube-flannel


Name:               kube-flannel-ds-amd64-c6xm2
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se061/10.1.2.61
Start Time:         Sat, 27 Apr 2019 03:48:25 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.61
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://f242f8d2d10dd9404e7643022081c74b0f3d3bfb4a88d38f18bc3f12f3ef275a
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:26 +0000
      Finished:     Sat, 27 Apr 2019 03:48:26 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://aac455fba833dc0ef1d6976a89d0bb4b5594ba6c8d4184cc952d085805ded50b
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:27 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-c6xm2 (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  5m18s  default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-c6xm2 to se061
  Normal  Pulled     5m17s  kubelet, se061     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m17s  kubelet, se061     Created container install-cni
  Normal  Started    5m17s  kubelet, se061     Started container install-cni
  Normal  Pulled     5m16s  kubelet, se061     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m16s  kubelet, se061     Created container kube-flannel
  Normal  Started    5m16s  kubelet, se061     Started container kube-flannel


Name:               kube-flannel-ds-amd64-czmz2
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se013/10.1.2.13
Start Time:         Sat, 27 Apr 2019 03:48:25 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.13
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://c27880a7861bbcb1a830efd33216a1c103ade0e2e07d9343bdf212c9445792cd
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:26 +0000
      Finished:     Sat, 27 Apr 2019 03:48:26 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://9b87c41d3569379a4934a5b839797622d24f0215dcf3e8bfab70edc556a78d70
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:27 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-czmz2 (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type     Reason            Age                  From               Message
  ----     ------            ----                 ----               -------
  Normal   Scheduled         5m18s                default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-czmz2 to se013
  Normal   Pulled            5m17s                kubelet, se013     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal   Created           5m17s                kubelet, se013     Created container install-cni
  Normal   Started           5m17s                kubelet, se013     Started container install-cni
  Normal   Started           5m16s                kubelet, se013     Started container kube-flannel
  Normal   Pulled            5m16s                kubelet, se013     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal   Created           5m16s                kubelet, se013     Created container kube-flannel
  Warning  DNSConfigForming  25s (x9 over 5m18s)  kubelet, se013     Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 147.8.176.15 147.8.178.15 147.8.175.12


Name:               kube-flannel-ds-amd64-czv4g
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se041/10.1.2.41
Start Time:         Sat, 27 Apr 2019 03:48:24 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.41
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://7416788e9056bdc7f05f129ed311fa8c40ac964f22d78c6b157701e2de0eb58f
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:25 +0000
      Finished:     Sat, 27 Apr 2019 03:48:25 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://a9100b5f1a89aaefccfa7690e3f420e9692b6d52bd3e49dc6cb7d04a6faf2560
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:26 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-czv4g (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  5m20s  default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-czv4g to se041
  Normal  Pulled     5m18s  kubelet, se041     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m18s  kubelet, se041     Created container install-cni
  Normal  Started    5m18s  kubelet, se041     Started container install-cni
  Normal  Pulled     5m17s  kubelet, se041     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m17s  kubelet, se041     Created container kube-flannel
  Normal  Started    5m17s  kubelet, se041     Started container kube-flannel


Name:               kube-flannel-ds-amd64-dbf55
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se039/10.1.2.39
Start Time:         Sat, 27 Apr 2019 03:48:24 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.39
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://08528abd8f19994eeb9175a8cd4587cd293ff9da2573385aa9fce05b71c42c12
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:25 +0000
      Finished:     Sat, 27 Apr 2019 03:48:25 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://e59e93894f72bf4dec3e3f6853b94dc3a55e8892871409da03fac8cc199f7690
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:26 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-dbf55 (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  5m20s  default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-dbf55 to se039
  Normal  Pulled     5m18s  kubelet, se039     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m18s  kubelet, se039     Created container install-cni
  Normal  Started    5m18s  kubelet, se039     Started container install-cni
  Normal  Pulled     5m17s  kubelet, se039     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m17s  kubelet, se039     Created container kube-flannel
  Normal  Started    5m17s  kubelet, se039     Started container kube-flannel


Name:               kube-flannel-ds-amd64-g4ftt
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se029/10.1.2.29
Start Time:         Sat, 27 Apr 2019 03:48:24 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.29
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://0a632710e86b6b1f7557235973cde51b86e6cd7a305727ec95e979d61235e14b
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:25 +0000
      Finished:     Sat, 27 Apr 2019 03:48:25 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://86750402efc974d76cd2ca4572ae80e69b0137289ff85ce0a7730be123e28c10
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:27 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-g4ftt (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  5m19s  default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-g4ftt to se029
  Normal  Pulled     5m18s  kubelet, se029     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m18s  kubelet, se029     Created container install-cni
  Normal  Started    5m18s  kubelet, se029     Started container install-cni
  Normal  Pulled     5m17s  kubelet, se029     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m17s  kubelet, se029     Created container kube-flannel
  Normal  Started    5m16s  kubelet, se029     Started container kube-flannel


Name:               kube-flannel-ds-amd64-g6f5z
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se005/10.1.2.5
Start Time:         Sat, 27 Apr 2019 03:48:25 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.5
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://c55b09f16371e0507f46644e6e825fe89d63f3606598095dd18e6e8514739a86
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:27 +0000
      Finished:     Sat, 27 Apr 2019 03:48:27 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://0d1618090ba03b9e9c6317fcf68229ea71164733d43dc342414e5bfc05a75e06
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:28 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-g6f5z (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type     Reason            Age                  From               Message
  ----     ------            ----                 ----               -------
  Normal   Scheduled         5m18s                default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-g6f5z to se005
  Normal   Pulled            5m17s                kubelet, se005     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal   Created           5m17s                kubelet, se005     Created container install-cni
  Normal   Started           5m16s                kubelet, se005     Started container install-cni
  Normal   Pulled            5m16s                kubelet, se005     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal   Created           5m16s                kubelet, se005     Created container kube-flannel
  Normal   Started           5m15s                kubelet, se005     Started container kube-flannel
  Warning  DNSConfigForming  46s (x9 over 5m18s)  kubelet, se005     Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 147.8.176.15 147.8.178.15 147.8.175.12


Name:               kube-flannel-ds-amd64-gdt7z
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se017/10.1.2.17
Start Time:         Sat, 27 Apr 2019 03:48:24 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.17
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://8f353e02dee635c9a59fe5b63766e335ac846d3b795167096bb0e7580bfb87a4
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:26 +0000
      Finished:     Sat, 27 Apr 2019 03:48:26 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://f68eb4c54bccf63aed11ce66c81b0a6d4ad67688d65d2cb7076122d6a702d710
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:27 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-gdt7z (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type     Reason            Age                  From               Message
  ----     ------            ----                 ----               -------
  Normal   Scheduled         5m19s                default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-gdt7z to se017
  Normal   Pulled            5m18s                kubelet, se017     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal   Created           5m18s                kubelet, se017     Created container install-cni
  Normal   Started           5m17s                kubelet, se017     Started container install-cni
  Normal   Pulled            5m17s                kubelet, se017     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal   Created           5m17s                kubelet, se017     Created container kube-flannel
  Normal   Started           5m16s                kubelet, se017     Started container kube-flannel
  Warning  DNSConfigForming  82s (x8 over 5m19s)  kubelet, se017     Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 147.8.176.15 147.8.178.15 147.8.175.12


Name:               kube-flannel-ds-amd64-gq99g
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se015/10.1.2.15
Start Time:         Sat, 27 Apr 2019 03:48:24 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.15
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://3ff7fa385ba6e61ace4d11fdacad2701ebc931d63109910cde41cdf52c7f6242
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:26 +0000
      Finished:     Sat, 27 Apr 2019 03:48:26 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://4e5078ced5e96099be239bde9b87619e2289060a7c6d049acc6dae11062e652e
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:27 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-gq99g (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type     Reason            Age                 From               Message
  ----     ------            ----                ----               -------
  Normal   Scheduled         5m20s               default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-gq99g to se015
  Normal   Pulled            5m17s               kubelet, se015     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal   Created           5m17s               kubelet, se015     Created container install-cni
  Normal   Started           5m17s               kubelet, se015     Started container install-cni
  Normal   Started           5m16s               kubelet, se015     Started container kube-flannel
  Normal   Pulled            5m16s               kubelet, se015     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal   Created           5m16s               kubelet, se015     Created container kube-flannel
  Warning  DNSConfigForming  4s (x9 over 5m18s)  kubelet, se015     Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 147.8.176.15 147.8.178.15 147.8.175.12


Name:               kube-flannel-ds-amd64-h82ss
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se037/10.1.2.37
Start Time:         Sat, 27 Apr 2019 03:48:24 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.37
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://0e5dd6d776641ed93a57d2c7bd8d72988883ff06e3c85b4f267c90d7791bd8d1
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:26 +0000
      Finished:     Sat, 27 Apr 2019 03:48:26 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://afbe16c493b123af68a754c8e5e51fe2a923b98719ccafabc3aac6b1e0015561
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:27 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-h82ss (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  5m19s  default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-h82ss to se037
  Normal  Pulled     5m18s  kubelet, se037     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m18s  kubelet, se037     Created container install-cni
  Normal  Started    5m17s  kubelet, se037     Started container install-cni
  Normal  Pulled     5m17s  kubelet, se037     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m17s  kubelet, se037     Created container kube-flannel
  Normal  Started    5m16s  kubelet, se037     Started container kube-flannel


Name:               kube-flannel-ds-amd64-hh6mf
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se058/10.1.2.58
Start Time:         Sat, 27 Apr 2019 03:48:24 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.58
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://9948de5da62465cf65c216a9e6de3c4d30975e3b9787dd6c2e7e2dd5775a7ae2
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:25 +0000
      Finished:     Sat, 27 Apr 2019 03:48:25 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://9eab246d4bc6ee5bc2f84e306706d52155319c9fc9b4b854c1245848cf5a92ce
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:26 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-hh6mf (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  5m19s  default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-hh6mf to se058
  Normal  Pulled     5m18s  kubelet, se058     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m18s  kubelet, se058     Created container install-cni
  Normal  Started    5m18s  kubelet, se058     Started container install-cni
  Normal  Pulled     5m17s  kubelet, se058     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m17s  kubelet, se058     Created container kube-flannel
  Normal  Started    5m17s  kubelet, se058     Started container kube-flannel


Name:               kube-flannel-ds-amd64-j49l7
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se033/10.1.2.33
Start Time:         Sat, 27 Apr 2019 03:48:25 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.33
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://b588fb7bc4c78998157195cf88942af59a6f209f05291a2dcfc858510caf3d7d
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:27 +0000
      Finished:     Sat, 27 Apr 2019 03:48:27 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://0eb01ea485d29dddc42db6ff409eb55e4b0d06286d5ae84cdeb588ea27e58cfb
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:28 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-j49l7 (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  5m18s  default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-j49l7 to se033
  Normal  Pulled     5m17s  kubelet, se033     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m16s  kubelet, se033     Created container install-cni
  Normal  Started    5m16s  kubelet, se033     Started container install-cni
  Normal  Pulled     5m16s  kubelet, se033     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m15s  kubelet, se033     Created container kube-flannel
  Normal  Started    5m15s  kubelet, se033     Started container kube-flannel


Name:               kube-flannel-ds-amd64-j8rtw
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se056/10.1.2.56
Start Time:         Sat, 27 Apr 2019 03:48:25 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.56
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://ca32e60f3cbfdcb17562086e168e95f940fb3be0477cc341b4341d8124354cad
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:26 +0000
      Finished:     Sat, 27 Apr 2019 03:48:26 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://94803aed9b36873889a6d5c20cfeaac4f48f4b7c34e54eb15e2ed9fff5b33c7a
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:28 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-j8rtw (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  5m18s  default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-j8rtw to se056
  Normal  Pulled     5m17s  kubelet, se056     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m17s  kubelet, se056     Created container install-cni
  Normal  Started    5m17s  kubelet, se056     Started container install-cni
  Normal  Pulled     5m16s  kubelet, se056     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m16s  kubelet, se056     Created container kube-flannel
  Normal  Started    5m15s  kubelet, se056     Started container kube-flannel


Name:               kube-flannel-ds-amd64-k69k5
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se006/10.1.2.6
Start Time:         Sat, 27 Apr 2019 03:48:25 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.6
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://3b8e2e742f2d697a178768ce0366856433416b82f5935cb4fab1b37fe689a10b
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:27 +0000
      Finished:     Sat, 27 Apr 2019 03:48:27 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://33d475703890c2da29b309e156f4276542f2d5370835910ae6f4ad0beebf76de
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:28 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-k69k5 (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type     Reason            Age                  From               Message
  ----     ------            ----                 ----               -------
  Normal   Scheduled         5m18s                default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-k69k5 to se006
  Normal   Pulled            5m17s                kubelet, se006     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal   Created           5m17s                kubelet, se006     Created container install-cni
  Normal   Started           5m16s                kubelet, se006     Started container install-cni
  Normal   Pulled            5m16s                kubelet, se006     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal   Created           5m16s                kubelet, se006     Created container kube-flannel
  Normal   Started           5m15s                kubelet, se006     Started container kube-flannel
  Warning  DNSConfigForming  37s (x9 over 5m18s)  kubelet, se006     Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 147.8.176.15 147.8.178.15 147.8.175.12


Name:               kube-flannel-ds-amd64-knswk
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se025/10.1.2.25
Start Time:         Sat, 27 Apr 2019 03:48:24 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.25
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://87c03926fc0881e58fd49203944f1a45b8827a0b398434bd60e00880b4f19270
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:25 +0000
      Finished:     Sat, 27 Apr 2019 03:48:25 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://2f13d3dbfa842c1cacbdc15ae0b732de67302ed654220cd070b3641e060c0c7b
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:26 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-knswk (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  5m20s  default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-knswk to se025
  Normal  Pulled     5m18s  kubelet, se025     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m18s  kubelet, se025     Created container install-cni
  Normal  Started    5m18s  kubelet, se025     Started container install-cni
  Normal  Pulled     5m17s  kubelet, se025     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m17s  kubelet, se025     Created container kube-flannel
  Normal  Started    5m17s  kubelet, se025     Started container kube-flannel


Name:               kube-flannel-ds-amd64-lkqls
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se057/10.1.2.57
Start Time:         Sat, 27 Apr 2019 03:48:24 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.57
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://3864c9bb4d8ea3a41dd727ccfcdfc0c8a6cca6106ca5ebbf50a6c7fe19dc5557
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:25 +0000
      Finished:     Sat, 27 Apr 2019 03:48:25 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://ef409051b8a3ffe8de369ef55dc106a97df5f85e6634730e97ccfb0eb7a054db
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:27 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-lkqls (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  5m20s  default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-lkqls to se057
  Normal  Pulled     5m18s  kubelet, se057     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m18s  kubelet, se057     Created container install-cni
  Normal  Started    5m18s  kubelet, se057     Started container install-cni
  Normal  Pulled     5m17s  kubelet, se057     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m17s  kubelet, se057     Created container kube-flannel
  Normal  Started    5m16s  kubelet, se057     Started container kube-flannel


Name:               kube-flannel-ds-amd64-lsrjh
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se012/10.1.2.12
Start Time:         Sat, 27 Apr 2019 03:48:25 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.12
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://05aee0a9b40517d9a524a86a9d1078d31dc959a13aa2350d32a5f1a7165b944d
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:26 +0000
      Finished:     Sat, 27 Apr 2019 03:48:26 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://c013bb23a5e50723296b9455af7e5b6f78bac971072454f258fd731cdc9b419c
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:28 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-lsrjh (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type     Reason            Age                  From               Message
  ----     ------            ----                 ----               -------
  Normal   Scheduled         5m18s                default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-lsrjh to se012
  Normal   Pulled            5m17s                kubelet, se012     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal   Created           5m17s                kubelet, se012     Created container install-cni
  Normal   Started           5m17s                kubelet, se012     Started container install-cni
  Normal   Pulled            5m16s                kubelet, se012     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal   Started           5m15s                kubelet, se012     Started container kube-flannel
  Normal   Created           5m15s                kubelet, se012     Created container kube-flannel
  Warning  DNSConfigForming  34s (x9 over 5m18s)  kubelet, se012     Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 147.8.176.15 147.8.178.15 147.8.175.12


Name:               kube-flannel-ds-amd64-lv24t
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se016/10.1.2.16
Start Time:         Sat, 27 Apr 2019 03:48:25 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.16
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://f88b46495ae370a4ea2299c31eceb472f2298f8dd31c6ff7081b4a451e40320b
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:27 +0000
      Finished:     Sat, 27 Apr 2019 03:48:27 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://2219318f5123f678f08337df04db2ca19e0411b08d860b4a5ed877273e9b6bfd
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:28 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-lv24t (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type     Reason            Age                  From               Message
  ----     ------            ----                 ----               -------
  Normal   Scheduled         5m18s                default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-lv24t to se016
  Normal   Pulled            5m17s                kubelet, se016     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal   Created           5m17s                kubelet, se016     Created container install-cni
  Normal   Started           5m16s                kubelet, se016     Started container install-cni
  Normal   Pulled            5m16s                kubelet, se016     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal   Created           5m16s                kubelet, se016     Created container kube-flannel
  Normal   Started           5m15s                kubelet, se016     Started container kube-flannel
  Warning  DNSConfigForming  30s (x9 over 5m18s)  kubelet, se016     Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 147.8.176.15 147.8.178.15 147.8.175.12


Name:               kube-flannel-ds-amd64-lvz86
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se034/10.1.2.34
Start Time:         Sat, 27 Apr 2019 03:48:25 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.34
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://f998f4f4b4a828c730c5c5b27584467449c8ec3d8665e49800ce09f7c00a9d0c
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:26 +0000
      Finished:     Sat, 27 Apr 2019 03:48:26 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://86939553ca3fea7a9a6aebcf8e61924de370fbbecdad435a37f6190a27e383c9
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:28 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-lvz86 (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  5m19s  default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-lvz86 to se034
  Normal  Pulled     5m17s  kubelet, se034     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m17s  kubelet, se034     Created container install-cni
  Normal  Started    5m17s  kubelet, se034     Started container install-cni
  Normal  Pulled     5m16s  kubelet, se034     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m16s  kubelet, se034     Created container kube-flannel
  Normal  Started    5m15s  kubelet, se034     Started container kube-flannel


Name:               kube-flannel-ds-amd64-mc7bw
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se022/10.1.2.22
Start Time:         Sat, 27 Apr 2019 03:48:24 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.22
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://7e042f49594e2659b7f25c9df99868b3441888b0c682c9ae309b52083613bf9c
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:25 +0000
      Finished:     Sat, 27 Apr 2019 03:48:25 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://c4c97ea7d3798e6b7ee2a981948f743294741f2b535aa00e7ed5fac57a0be7d4
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:27 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-mc7bw (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type     Reason            Age                  From               Message
  ----     ------            ----                 ----               -------
  Normal   Scheduled         5m19s                default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-mc7bw to se022
  Normal   Pulled            5m18s                kubelet, se022     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal   Created           5m18s                kubelet, se022     Created container install-cni
  Normal   Started           5m18s                kubelet, se022     Started container install-cni
  Normal   Pulled            5m17s                kubelet, se022     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal   Created           5m17s                kubelet, se022     Created container kube-flannel
  Normal   Started           5m16s                kubelet, se022     Started container kube-flannel
  Warning  DNSConfigForming  10s (x9 over 5m19s)  kubelet, se022     Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 147.8.176.15 147.8.178.15 147.8.175.12


Name:               kube-flannel-ds-amd64-n545j
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se028/10.1.2.28
Start Time:         Sat, 27 Apr 2019 03:48:24 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.28
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://15e5deb2ba65d941671653cdfcacfaee01b05aec20e8bfa8864c39ba7e32d7c0
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:26 +0000
      Finished:     Sat, 27 Apr 2019 03:48:26 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://2c174e1c57edb928f5de0a6004d2c1cf872f414a1ca829bdb28b2e2ee8eaed1d
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:27 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-n545j (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  5m19s  default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-n545j to se028
  Normal  Pulled     5m17s  kubelet, se028     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m17s  kubelet, se028     Created container install-cni
  Normal  Started    5m17s  kubelet, se028     Started container install-cni
  Normal  Pulled     5m16s  kubelet, se028     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m16s  kubelet, se028     Created container kube-flannel
  Normal  Started    5m16s  kubelet, se028     Started container kube-flannel


Name:               kube-flannel-ds-amd64-n5chg
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se021/10.1.2.21
Start Time:         Sat, 27 Apr 2019 03:48:24 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.21
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://73376774debb5b6096812343565794ce6622c72d16f6ea9dd36ff2ba7fe04c44
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:25 +0000
      Finished:     Sat, 27 Apr 2019 03:48:25 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://a157e584ffa6748a15cc45c0ccb5d148cd9c16f0d4641feec1ce2fb6aea408bc
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:26 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-n5chg (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type     Reason            Age                  From               Message
  ----     ------            ----                 ----               -------
  Normal   Scheduled         5m20s                default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-n5chg to se021
  Normal   Pulled            5m18s                kubelet, se021     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal   Created           5m18s                kubelet, se021     Created container install-cni
  Normal   Started           5m18s                kubelet, se021     Started container install-cni
  Normal   Started           5m17s                kubelet, se021     Started container kube-flannel
  Normal   Pulled            5m17s                kubelet, se021     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal   Created           5m17s                kubelet, se021     Created container kube-flannel
  Warning  DNSConfigForming  10s (x9 over 5m19s)  kubelet, se021     Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 147.8.176.15 147.8.178.15 147.8.175.12


Name:               kube-flannel-ds-amd64-nj6fz
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se024/10.1.2.24
Start Time:         Sat, 27 Apr 2019 03:48:24 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.24
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://2c10857b3d63066c05770e3773b809e2822ec6cdd6bfc354f4856774602b2c0f
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:25 +0000
      Finished:     Sat, 27 Apr 2019 03:48:25 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://88a15c4f263fd39c75e8d373f39510ff68b488643eec8eaf77277d669e54b9c2
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:26 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-nj6fz (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  5m20s  default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-nj6fz to se024
  Normal  Pulled     5m18s  kubelet, se024     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m18s  kubelet, se024     Created container install-cni
  Normal  Started    5m18s  kubelet, se024     Started container install-cni
  Normal  Pulled     5m17s  kubelet, se024     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m17s  kubelet, se024     Created container kube-flannel
  Normal  Started    5m17s  kubelet, se024     Started container kube-flannel


Name:               kube-flannel-ds-amd64-p45tf
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se062/10.1.2.62
Start Time:         Sat, 27 Apr 2019 03:48:26 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.62
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://e549ebb080bc8e62cf455e374b89dc53efaaca3d836dab2233adc6ea6d5307ef
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:27 +0000
      Finished:     Sat, 27 Apr 2019 03:48:27 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://a23ac8740c05036e4c2d26e94e86b82eb2ce998ad866a391c6b08a0bb6c2101c
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:28 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-p45tf (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  5m17s  default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-p45tf to se062
  Normal  Pulled     5m16s  kubelet, se062     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m16s  kubelet, se062     Created container install-cni
  Normal  Started    5m16s  kubelet, se062     Started container install-cni
  Normal  Pulled     5m15s  kubelet, se062     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m15s  kubelet, se062     Created container kube-flannel
  Normal  Started    5m15s  kubelet, se062     Started container kube-flannel


Name:               kube-flannel-ds-amd64-p4f8c
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se038/10.1.2.38
Start Time:         Sat, 27 Apr 2019 03:48:24 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.38
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://bc81fb7ce0708da0faf5114e25a8aca775c94ef4d2fcbd3c3b5d08fd2c7bb685
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:26 +0000
      Finished:     Sat, 27 Apr 2019 03:48:26 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://cd13e67762b9246ed0387df7991a50c76763f19032dc9341a3159b438a8a6525
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:27 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-p4f8c (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  5m20s  default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-p4f8c to se038
  Normal  Pulled     5m19s  kubelet, se038     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m19s  kubelet, se038     Created container install-cni
  Normal  Started    5m18s  kubelet, se038     Started container install-cni
  Normal  Pulled     5m18s  kubelet, se038     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m18s  kubelet, se038     Created container kube-flannel
  Normal  Started    5m17s  kubelet, se038     Started container kube-flannel


Name:               kube-flannel-ds-amd64-p5trf
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se036/10.1.2.36
Start Time:         Sat, 27 Apr 2019 03:48:24 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.36
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://bd70bcf5522d3050a8a9df3267e0d9a891a4bad8b61b94b594675ab8a1369067
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:25 +0000
      Finished:     Sat, 27 Apr 2019 03:48:25 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://e758032459f2e195c5cab59051db5fe11fbc18438337077b166d92396b68198b
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:26 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-p5trf (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  5m21s  default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-p5trf to se036
  Normal  Pulled     5m19s  kubelet, se036     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m19s  kubelet, se036     Created container install-cni
  Normal  Started    5m19s  kubelet, se036     Started container install-cni
  Normal  Pulled     5m18s  kubelet, se036     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m18s  kubelet, se036     Created container kube-flannel
  Normal  Started    5m18s  kubelet, se036     Started container kube-flannel


Name:               kube-flannel-ds-amd64-pf4nt
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se001/10.1.2.1
Start Time:         Sat, 27 Apr 2019 03:48:23 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.1
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://e0ae8292501123fe9b7b84877fb9e04092ac8f2722eeea6056196753179020cd
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:25 +0000
      Finished:     Sat, 27 Apr 2019 03:48:25 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://e44136b21b19c514afbfa4116d22447ad1153fd1791d74ab99424bcdf2ee3444
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:26 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-pf4nt (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type     Reason            Age                  From               Message
  ----     ------            ----                 ----               -------
  Normal   Scheduled         5m21s                default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-pf4nt to se001
  Normal   Pulled            5m19s                kubelet, se001     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal   Created           5m19s                kubelet, se001     Created container install-cni
  Normal   Started           5m19s                kubelet, se001     Started container install-cni
  Normal   Started           5m18s                kubelet, se001     Started container kube-flannel
  Normal   Pulled            5m18s                kubelet, se001     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal   Created           5m18s                kubelet, se001     Created container kube-flannel
  Warning  DNSConfigForming  21s (x9 over 5m20s)  kubelet, se001     Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 147.8.176.15 147.8.178.15 147.8.175.12


Name:               kube-flannel-ds-amd64-prd5n
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se064/10.1.2.64
Start Time:         Sat, 27 Apr 2019 03:48:25 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.64
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://f16539f26f1a4fca6b42c1cbfdb2b6dc374476c8ef15ff2be9bd6b411460aae2
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:27 +0000
      Finished:     Sat, 27 Apr 2019 03:48:27 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://1cda4d83c6f708e59a8dceab0140189fa99e01236d9353172d7406a5ed0f5976
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:28 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-prd5n (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  5m19s  default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-prd5n to se064
  Normal  Pulled     5m18s  kubelet, se064     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m18s  kubelet, se064     Created container install-cni
  Normal  Started    5m17s  kubelet, se064     Started container install-cni
  Normal  Pulled     5m17s  kubelet, se064     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m17s  kubelet, se064     Created container kube-flannel
  Normal  Started    5m16s  kubelet, se064     Started container kube-flannel


Name:               kube-flannel-ds-amd64-q7hc4
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se002/10.1.2.2
Start Time:         Sat, 27 Apr 2019 03:48:23 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.2
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://caa1a0b44a7fcc99ea0f5d8dfe7378f955223b79f58a163a9fb56d806744a413
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:25 +0000
      Finished:     Sat, 27 Apr 2019 03:48:25 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://8c625957f4f66c156bd94bc0833eb8f115ed1fdbe115b5b30b14831b96aa4517
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:26 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-q7hc4 (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type     Reason            Age                  From               Message
  ----     ------            ----                 ----               -------
  Normal   Scheduled         5m21s                default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-q7hc4 to se002
  Normal   Pulled            5m19s                kubelet, se002     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal   Created           5m19s                kubelet, se002     Created container install-cni
  Normal   Started           5m19s                kubelet, se002     Started container install-cni
  Normal   Started           5m18s                kubelet, se002     Started container kube-flannel
  Normal   Pulled            5m18s                kubelet, se002     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal   Created           5m18s                kubelet, se002     Created container kube-flannel
  Warning  DNSConfigForming  20s (x9 over 5m20s)  kubelet, se002     Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 147.8.176.15 147.8.178.15 147.8.175.12


Name:               kube-flannel-ds-amd64-qpxq6
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se047/10.1.2.47
Start Time:         Sat, 27 Apr 2019 03:48:23 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.47
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://708de7767a5ffb49ab11c4f11bf6b92f96db1cb9e0f7952e9ed330e0e8dd98b7
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:25 +0000
      Finished:     Sat, 27 Apr 2019 03:48:25 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://b1fe611e677943cf5c67fff6d5e78afcad7b3a930e9155fa12195d1c24f2b111
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:26 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-qpxq6 (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  5m21s  default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-qpxq6 to se047
  Normal  Pulled     5m19s  kubelet, se047     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m19s  kubelet, se047     Created container install-cni
  Normal  Started    5m19s  kubelet, se047     Started container install-cni
  Normal  Pulled     5m18s  kubelet, se047     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m18s  kubelet, se047     Created container kube-flannel
  Normal  Started    5m18s  kubelet, se047     Started container kube-flannel


Name:               kube-flannel-ds-amd64-qt2sb
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se051/10.1.2.51
Start Time:         Sat, 27 Apr 2019 03:48:24 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.51
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://014bd252a59978888c2b8dcb0c825c1bb1889658543ee938576982da32e5c1af
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:26 +0000
      Finished:     Sat, 27 Apr 2019 03:48:26 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://9adf1cba0b2f50fc2b2ad6f3841f9d78ee6b53dd2f41a780f0c53ad0636082d4
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:28 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-qt2sb (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  5m21s  default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-qt2sb to se051
  Normal  Pulled     5m18s  kubelet, se051     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m18s  kubelet, se051     Created container install-cni
  Normal  Started    5m18s  kubelet, se051     Started container install-cni
  Normal  Pulled     5m17s  kubelet, se051     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m17s  kubelet, se051     Created container kube-flannel
  Normal  Started    5m16s  kubelet, se051     Started container kube-flannel


Name:               kube-flannel-ds-amd64-qtfpx
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se049/10.1.2.49
Start Time:         Sat, 27 Apr 2019 03:48:24 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.49
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://61b44a581ed483e95b00f20683782f4b3765dfc97e02324b21f6382a83ee4836
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:25 +0000
      Finished:     Sat, 27 Apr 2019 03:48:25 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://b194cd6a1f1ef0b862659eb036dd62fb52a609b44a127807e993af621d741e41
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:26 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-qtfpx (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  5m21s  default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-qtfpx to se049
  Normal  Pulled     5m19s  kubelet, se049     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m19s  kubelet, se049     Created container install-cni
  Normal  Started    5m19s  kubelet, se049     Started container install-cni
  Normal  Pulled     5m18s  kubelet, se049     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m18s  kubelet, se049     Created container kube-flannel
  Normal  Started    5m18s  kubelet, se049     Started container kube-flannel


Name:               kube-flannel-ds-amd64-qvj9k
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se050/10.1.2.50
Start Time:         Sat, 27 Apr 2019 03:48:24 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.50
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://4954462af010ffbcda32d5500754191a243fca16fb23f63d02ca353add2a1c4c
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:25 +0000
      Finished:     Sat, 27 Apr 2019 03:48:25 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://88b779a3d9e0325c7787b3675dce36162a28c52810ebe0d0722eb959e4df9bcf
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:27 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-qvj9k (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  5m21s  default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-qvj9k to se050
  Normal  Pulled     5m19s  kubelet, se050     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m19s  kubelet, se050     Created container install-cni
  Normal  Started    5m19s  kubelet, se050     Started container install-cni
  Normal  Pulled     5m18s  kubelet, se050     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m17s  kubelet, se050     Created container kube-flannel
  Normal  Started    5m17s  kubelet, se050     Started container kube-flannel


Name:               kube-flannel-ds-amd64-r7qg7
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se054/10.1.2.54
Start Time:         Sat, 27 Apr 2019 03:48:23 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.54
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://25e33c90635b7a3203f1f1b3a8c063b3aff8b99b63044453294af781e36cae13
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:25 +0000
      Finished:     Sat, 27 Apr 2019 03:48:25 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://972ee36289723829dd2ce177e36974b2e8a678e1e64c31ec730caff4d47fd3cc
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:26 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-r7qg7 (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  5m21s  default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-r7qg7 to se054
  Normal  Pulled     5m19s  kubelet, se054     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m19s  kubelet, se054     Created container install-cni
  Normal  Started    5m19s  kubelet, se054     Started container install-cni
  Normal  Pulled     5m18s  kubelet, se054     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m18s  kubelet, se054     Created container kube-flannel
  Normal  Started    5m18s  kubelet, se054     Started container kube-flannel


Name:               kube-flannel-ds-amd64-rdnsq
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se026/10.1.2.26
Start Time:         Sat, 27 Apr 2019 03:48:25 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.26
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://28399ea25cf244fb40c61eec7b7bd39603e58fe8dd27a31ba0ab803bab468e7d
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:27 +0000
      Finished:     Sat, 27 Apr 2019 03:48:27 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://e43d805fd0a99de160ee9a4f7516d256e8645c53f753fbfdc728b3703ae0b4b6
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:29 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-rdnsq (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  5m19s  default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-rdnsq to se026
  Normal  Pulled     5m17s  kubelet, se026     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m17s  kubelet, se026     Created container install-cni
  Normal  Started    5m17s  kubelet, se026     Started container install-cni
  Normal  Pulled     5m16s  kubelet, se026     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m16s  kubelet, se026     Created container kube-flannel
  Normal  Started    5m15s  kubelet, se026     Started container kube-flannel


Name:               kube-flannel-ds-amd64-rxjgg
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se011/10.1.2.11
Start Time:         Sat, 27 Apr 2019 03:48:25 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.11
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://0f376444ade435ebadf747dc15b37aeb91789375c41186aa8d6caeee0eed22e5
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:27 +0000
      Finished:     Sat, 27 Apr 2019 03:48:27 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://376269daef4d56a4b020180819e1e37057d7b2e8066429ea5291cedecbd7d190
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:28 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-rxjgg (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type     Reason            Age                  From               Message
  ----     ------            ----                 ----               -------
  Normal   Scheduled         5m19s                default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-rxjgg to se011
  Normal   Pulled            5m18s                kubelet, se011     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal   Created           5m18s                kubelet, se011     Created container install-cni
  Normal   Started           5m17s                kubelet, se011     Started container install-cni
  Normal   Pulled            5m17s                kubelet, se011     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal   Created           5m17s                kubelet, se011     Created container kube-flannel
  Normal   Started           5m16s                kubelet, se011     Started container kube-flannel
  Warning  DNSConfigForming  11s (x9 over 5m19s)  kubelet, se011     Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 147.8.176.15 147.8.178.15 147.8.175.12


Name:               kube-flannel-ds-amd64-s2rr9
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se030/10.1.2.30
Start Time:         Sat, 27 Apr 2019 03:48:23 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.30
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://88270346d4654ef8cf7b75fc22733367639bf093cb7a45991b8e1b5693da3595
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:25 +0000
      Finished:     Sat, 27 Apr 2019 03:48:25 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://123aa4d5e12a1cdd705494a45402b0a6542e2f1aeed4df016e6182d2d5f2dfce
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:26 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-s2rr9 (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  5m21s  default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-s2rr9 to se030
  Normal  Pulled     5m19s  kubelet, se030     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m19s  kubelet, se030     Created container install-cni
  Normal  Started    5m19s  kubelet, se030     Started container install-cni
  Normal  Pulled     5m18s  kubelet, se030     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m18s  kubelet, se030     Created container kube-flannel
  Normal  Started    5m18s  kubelet, se030     Started container kube-flannel


Name:               kube-flannel-ds-amd64-s9rxj
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se035/10.1.2.35
Start Time:         Sat, 27 Apr 2019 03:48:25 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.35
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://2998a7d3968d935817ce5c67adc84de4cea9c451f44d4d4d8fa23eecdefbbc1d
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:27 +0000
      Finished:     Sat, 27 Apr 2019 03:48:27 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://5257967c81af9571874f01b242e2b6f1775a3324a6f2e034369dfce76736261f
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:28 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-s9rxj (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  5m19s  default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-s9rxj to se035
  Normal  Pulled     5m18s  kubelet, se035     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m18s  kubelet, se035     Created container install-cni
  Normal  Started    5m17s  kubelet, se035     Started container install-cni
  Normal  Pulled     5m17s  kubelet, se035     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m17s  kubelet, se035     Created container kube-flannel
  Normal  Started    5m16s  kubelet, se035     Started container kube-flannel


Name:               kube-flannel-ds-amd64-slcgl
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se027/10.1.2.27
Start Time:         Sat, 27 Apr 2019 03:48:24 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.27
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://b6829d9db997de62dec74d16577a673be187a48bfb185f65191dbb4f024de442
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:25 +0000
      Finished:     Sat, 27 Apr 2019 03:48:25 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://812c74279cb80b26b862ac87b4ab2f2c7e82e5334bef61d5c3a9794ddc3a69b9
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:27 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-slcgl (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  5m21s  default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-slcgl to se027
  Normal  Pulled     5m19s  kubelet, se027     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m19s  kubelet, se027     Created container install-cni
  Normal  Started    5m19s  kubelet, se027     Started container install-cni
  Normal  Pulled     5m18s  kubelet, se027     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m18s  kubelet, se027     Created container kube-flannel
  Normal  Started    5m17s  kubelet, se027     Started container kube-flannel


Name:               kube-flannel-ds-amd64-sndvm
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se023/10.1.2.23
Start Time:         Sat, 27 Apr 2019 03:48:24 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.23
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://c28e0829867eb71d8a12244d3dfb85b0096cf5e499d08769c6c001b77b1c95ac
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:26 +0000
      Finished:     Sat, 27 Apr 2019 03:48:26 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://b25dd1934d4d7faed3d750611df3a4fbcb2a3025c82bc2b065ff18783c936b85
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:27 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-sndvm (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type     Reason            Age                  From               Message
  ----     ------            ----                 ----               -------
  Normal   Scheduled         5m21s                default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-sndvm to se023
  Normal   Pulled            5m19s                kubelet, se023     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal   Created           5m18s                kubelet, se023     Created container install-cni
  Normal   Started           5m18s                kubelet, se023     Started container install-cni
  Normal   Pulled            5m18s                kubelet, se023     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal   Started           5m17s                kubelet, se023     Started container kube-flannel
  Normal   Created           5m17s                kubelet, se023     Created container kube-flannel
  Warning  DNSConfigForming  19s (x9 over 5m19s)  kubelet, se023     Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 147.8.176.15 147.8.178.15 147.8.175.12


Name:               kube-flannel-ds-amd64-tj85p
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se063/10.1.2.63
Start Time:         Sat, 27 Apr 2019 03:48:24 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.63
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://b19eef19f1e0d47cef60d361cb9e16657b57b961228fa8d08fdc96ca6b93c6eb
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:26 +0000
      Finished:     Sat, 27 Apr 2019 03:48:26 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://1a07a5172d027f1aaa2fad2980fe775617944c8a18b4c617f313ebd1ab272e85
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:26 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-tj85p (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  5m20s  default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-tj85p to se063
  Normal  Pulled     5m19s  kubelet, se063     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m19s  kubelet, se063     Created container install-cni
  Normal  Started    5m18s  kubelet, se063     Started container install-cni
  Normal  Pulled     5m18s  kubelet, se063     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m18s  kubelet, se063     Created container kube-flannel
  Normal  Started    5m18s  kubelet, se063     Started container kube-flannel


Name:               kube-flannel-ds-amd64-v4qrl
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se018/10.1.2.18
Start Time:         Sat, 27 Apr 2019 03:48:23 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.18
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://22dcba2b318862ab9dae581ef8ca0d991589e7dd0f8134873d0e066a828630e5
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:25 +0000
      Finished:     Sat, 27 Apr 2019 03:48:25 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://f4ac2154580a101d9af7b8d9f8480c60cc219add7a6734d8e2eb1e9b94665b59
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:27 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-v4qrl (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type     Reason            Age                  From               Message
  ----     ------            ----                 ----               -------
  Normal   Scheduled         5m21s                default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-v4qrl to se018
  Normal   Pulled            5m20s                kubelet, se018     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal   Created           5m19s                kubelet, se018     Created container install-cni
  Normal   Started           5m19s                kubelet, se018     Started container install-cni
  Normal   Pulled            5m18s                kubelet, se018     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal   Created           5m18s                kubelet, se018     Created container kube-flannel
  Normal   Started           5m17s                kubelet, se018     Started container kube-flannel
  Warning  DNSConfigForming  24s (x9 over 5m20s)  kubelet, se018     Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 147.8.176.15 147.8.178.15 147.8.175.12


Name:               kube-flannel-ds-amd64-v5dw4
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se007/10.1.2.7
Start Time:         Sat, 27 Apr 2019 03:48:24 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.7
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://f4488453beff77ae58e858ad8bb9975e5b257adcf5f65997b2ce91aa18fdb46e
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:26 +0000
      Finished:     Sat, 27 Apr 2019 03:48:26 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://08794746dc5a178e187ae1ad2682451d4f1a7038130dd9311cc56ac321d446a7
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:27 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-v5dw4 (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type     Reason            Age                  From               Message
  ----     ------            ----                 ----               -------
  Normal   Scheduled         5m20s                default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-v5dw4 to se007
  Normal   Pulled            5m19s                kubelet, se007     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal   Created           5m18s                kubelet, se007     Created container install-cni
  Normal   Started           5m18s                kubelet, se007     Started container install-cni
  Normal   Started           5m17s                kubelet, se007     Started container kube-flannel
  Normal   Pulled            5m17s                kubelet, se007     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal   Created           5m17s                kubelet, se007     Created container kube-flannel
  Warning  DNSConfigForming  22s (x9 over 5m19s)  kubelet, se007     Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 147.8.176.15 147.8.178.15 147.8.175.12


Name:               kube-flannel-ds-amd64-v679p
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se052/10.1.2.52
Start Time:         Sat, 27 Apr 2019 03:48:26 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.52
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://d136f6436eba40356eaf70042375469f3271fd7f52cb3d36d4e0d68ab6f40677
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:27 +0000
      Finished:     Sat, 27 Apr 2019 03:48:27 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://1b8bb34af63f4b009127af3a626dadc4d228dbaf67e41bf6e517c6c38a5a1d1e
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:28 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-v679p (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  5m19s  default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-v679p to se052
  Normal  Pulled     5m17s  kubelet, se052     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m17s  kubelet, se052     Created container install-cni
  Normal  Started    5m17s  kubelet, se052     Started container install-cni
  Normal  Pulled     5m16s  kubelet, se052     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m16s  kubelet, se052     Created container kube-flannel
  Normal  Started    5m16s  kubelet, se052     Started container kube-flannel


Name:               kube-flannel-ds-amd64-vkm9f
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se055/10.1.2.55
Start Time:         Sat, 27 Apr 2019 03:48:26 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.55
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://044bad2fc27cfb232c7df4a16248aa1d97aedf9195a415abde2877e1fe1cadbc
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:27 +0000
      Finished:     Sat, 27 Apr 2019 03:48:27 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://5a260c9ec5e1fcd51b5cd04a6de3b86d2a9526a9866668e69d3e28f1266a2af8
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:29 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-vkm9f (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  5m18s  default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-vkm9f to se055
  Normal  Pulled     5m17s  kubelet, se055     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m17s  kubelet, se055     Created container install-cni
  Normal  Started    5m17s  kubelet, se055     Started container install-cni
  Normal  Pulled     5m16s  kubelet, se055     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m16s  kubelet, se055     Created container kube-flannel
  Normal  Started    5m15s  kubelet, se055     Started container kube-flannel


Name:               kube-flannel-ds-amd64-vkpnk
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se059/10.1.2.59
Start Time:         Sat, 27 Apr 2019 03:48:24 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.59
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://97ade57d3b2739fa92fbcd1bbf2131853da7fda626fbc321db44d6b8c6f4cc5f
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:25 +0000
      Finished:     Sat, 27 Apr 2019 03:48:25 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://b8f8b05c179e46790f8af9a7b3b4ee48a8fd66b35bfea0f783fc935c78abcced
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:26 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-vkpnk (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  5m20s  default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-vkpnk to se059
  Normal  Pulled     5m19s  kubelet, se059     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m19s  kubelet, se059     Created container install-cni
  Normal  Started    5m19s  kubelet, se059     Started container install-cni
  Normal  Pulled     5m18s  kubelet, se059     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m18s  kubelet, se059     Created container kube-flannel
  Normal  Started    5m18s  kubelet, se059     Started container kube-flannel


Name:               kube-flannel-ds-amd64-vqg66
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se043/10.1.2.43
Start Time:         Sat, 27 Apr 2019 03:48:24 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.43
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://abadab97eed652ffa294186ab93a8e64dae0aac20fb30dab826381c79f0c9668
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:25 +0000
      Finished:     Sat, 27 Apr 2019 03:48:25 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://1b0b97eb21ef054f4159146327ca9c76e801394d6978c356f8235675d55d65f3
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:26 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-vqg66 (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  5m21s  default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-vqg66 to se043
  Normal  Pulled     5m19s  kubelet, se043     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m19s  kubelet, se043     Created container install-cni
  Normal  Started    5m19s  kubelet, se043     Started container install-cni
  Normal  Pulled     5m18s  kubelet, se043     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m18s  kubelet, se043     Created container kube-flannel
  Normal  Started    5m18s  kubelet, se043     Started container kube-flannel


Name:               kube-flannel-ds-amd64-vxkgz
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se019/10.1.2.19
Start Time:         Sat, 27 Apr 2019 03:48:25 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.19
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://3acda392f281268bcbd97b51825367a108d288e600d1587880a9dc41c361de22
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:26 +0000
      Finished:     Sat, 27 Apr 2019 03:48:26 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://f0d80c5dcb4fc97abc92fa09e6ef13c7677bda64e566bd7cc1fd1eb27bcaed2a
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:28 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-vxkgz (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type     Reason            Age                  From               Message
  ----     ------            ----                 ----               -------
  Normal   Scheduled         5m19s                default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-vxkgz to se019
  Normal   Pulled            5m18s                kubelet, se019     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal   Created           5m18s                kubelet, se019     Created container install-cni
  Normal   Started           5m18s                kubelet, se019     Started container install-cni
  Normal   Pulled            5m17s                kubelet, se019     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal   Started           5m16s                kubelet, se019     Started container kube-flannel
  Normal   Created           5m16s                kubelet, se019     Created container kube-flannel
  Warning  DNSConfigForming  13s (x9 over 5m19s)  kubelet, se019     Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 147.8.176.15 147.8.178.15 147.8.175.12


Name:               kube-flannel-ds-amd64-wqcxx
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se003/10.1.2.3
Start Time:         Sat, 27 Apr 2019 03:48:24 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.3
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://41dc9b5b83b975f1c2b24a6da2708de7be0d7311c874c2907e2e40513ff93a1e
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:25 +0000
      Finished:     Sat, 27 Apr 2019 03:48:25 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://5d2705fe5cbb22f20a8cc987605d1305774a824028368e8ba54bd229265e7b1e
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:26 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-wqcxx (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type     Reason            Age                  From               Message
  ----     ------            ----                 ----               -------
  Normal   Scheduled         5m20s                default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-wqcxx to se003
  Normal   Pulled            5m19s                kubelet, se003     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal   Created           5m19s                kubelet, se003     Created container install-cni
  Normal   Started           5m19s                kubelet, se003     Started container install-cni
  Normal   Started           5m18s                kubelet, se003     Started container kube-flannel
  Normal   Pulled            5m18s                kubelet, se003     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal   Created           5m18s                kubelet, se003     Created container kube-flannel
  Warning  DNSConfigForming  76s (x8 over 5m20s)  kubelet, se003     Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 147.8.176.15 147.8.178.15 147.8.175.12


Name:               kube-flannel-ds-amd64-xttw8
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se046/10.1.2.46
Start Time:         Sat, 27 Apr 2019 03:48:25 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.46
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://5dcbe98ca0b99a4c9faccd5330dacc7bc02a34a97214bb30482bf3e718ff226f
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:26 +0000
      Finished:     Sat, 27 Apr 2019 03:48:27 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://412a986059b0bb94f7baf228c6180411b77d775ac5310cd488620e4118b08d2d
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:28 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-xttw8 (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  5m19s  default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-xttw8 to se046
  Normal  Pulled     5m18s  kubelet, se046     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m18s  kubelet, se046     Created container install-cni
  Normal  Started    5m17s  kubelet, se046     Started container install-cni
  Normal  Pulled     5m16s  kubelet, se046     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal  Created    5m16s  kubelet, se046     Created container kube-flannel
  Normal  Started    5m16s  kubelet, se046     Started container kube-flannel


Name:               kube-flannel-ds-amd64-zfsgz
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se010/10.1.2.10
Start Time:         Sat, 27 Apr 2019 03:48:24 +0000
Labels:             app=flannel
                    controller-revision-hash=8676477c4
                    pod-template-generation=1
                    tier=node
Annotations:        <none>
Status:             Running
IP:                 10.1.2.10
Controlled By:      DaemonSet/kube-flannel-ds-amd64
Init Containers:
  install-cni:
    Container ID:  docker://f9dba45804d1e73211bdbfa76965479cee30cc2d57fb8b1e88417140575f6ca1
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      cp
    Args:
      -f
      /etc/kube-flannel/cni-conf.json
      /etc/cni/net.d/10-flannel.conflist
    State:          Terminated
      Reason:       Completed
      Exit Code:    0
      Started:      Sat, 27 Apr 2019 03:48:25 +0000
      Finished:     Sat, 27 Apr 2019 03:48:26 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /etc/cni/net.d from cni (rw)
      /etc/kube-flannel/ from flannel-cfg (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Containers:
  kube-flannel:
    Container ID:  docker://7757cd1960ca74066f375436ee7af2509e134b91ceada1c1b828016cf6bc65d1
    Image:         quay.io/coreos/flannel:v0.11.0-amd64
    Image ID:      docker-pullable://quay.io/coreos/flannel@sha256:7806805c93b20a168d0bbbd25c6a213f00ac58a511c47e8fa6409543528a204e
    Port:          <none>
    Host Port:     <none>
    Command:
      /opt/bin/flanneld
    Args:
      --ip-masq
      --kube-subnet-mgr
    State:          Running
      Started:      Sat, 27 Apr 2019 03:48:26 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  50Mi
    Requests:
      cpu:     100m
      memory:  50Mi
    Environment:
      POD_NAME:       kube-flannel-ds-amd64-zfsgz (v1:metadata.name)
      POD_NAMESPACE:  kube-system (v1:metadata.namespace)
    Mounts:
      /etc/kube-flannel/ from flannel-cfg (rw)
      /run/flannel from run (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from flannel-token-k75wv (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  run:
    Type:          HostPath (bare host directory volume)
    Path:          /run/flannel
    HostPathType:  
  cni:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/cni/net.d
    HostPathType:  
  flannel-cfg:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-flannel-cfg
    Optional:  false
  flannel-token-k75wv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  flannel-token-k75wv
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  beta.kubernetes.io/arch=amd64
Tolerations:     :NoSchedule
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type     Reason            Age                  From               Message
  ----     ------            ----                 ----               -------
  Normal   Scheduled         5m20s                default-scheduler  Successfully assigned kube-system/kube-flannel-ds-amd64-zfsgz to se010
  Normal   Pulled            5m19s                kubelet, se010     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal   Created           5m19s                kubelet, se010     Created container install-cni
  Normal   Started           5m18s                kubelet, se010     Started container kube-flannel
  Normal   Started           5m18s                kubelet, se010     Started container install-cni
  Normal   Pulled            5m18s                kubelet, se010     Container image "quay.io/coreos/flannel:v0.11.0-amd64" already present on machine
  Normal   Created           5m18s                kubelet, se010     Created container kube-flannel
  Warning  DNSConfigForming  16s (x9 over 5m20s)  kubelet, se010     Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 147.8.176.15 147.8.178.15 147.8.175.12


Name:               kube-proxy-22k28
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se006/10.1.2.6
Start Time:         Fri, 26 Apr 2019 04:25:27 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.6
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://fd7c72f1821d0890149a92c8b62e973b6febff0ea7dce2f8db1aa4aa7d73434a
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:25:28 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type     Reason            Age                     From            Message
  ----     ------            ----                    ----            -------
  Warning  DNSConfigForming  3m10s (x1124 over 23h)  kubelet, se006  Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 147.8.176.15 147.8.178.15 147.8.175.12


Name:               kube-proxy-2wpsw
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se039/10.1.2.39
Start Time:         Fri, 26 Apr 2019 04:32:31 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.39
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://3ad767b30514b72986bdeed9ca96648f82a6902d202d9e2e0851349772c89f74
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:32:33 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:          <none>


Name:               kube-proxy-4prw7
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se041/10.1.2.41
Start Time:         Fri, 26 Apr 2019 04:33:03 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.41
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://49bd1a8cccea1d0bd2530f8dfc8f278d98b75dc19477238c0d9bdbbc2e4524de
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:33:04 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:          <none>


Name:               kube-proxy-5d2z4
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se044/10.1.2.44
Start Time:         Fri, 26 Apr 2019 04:33:36 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.44
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://8208e74de6a00bf606b5db16f9d92fa5b9c4ab00d2844234ecf0759147a45322
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:33:38 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:          <none>


Name:               kube-proxy-5vqs6
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se025/10.1.2.25
Start Time:         Fri, 26 Apr 2019 04:29:55 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.25
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://507fb6162113b477e385d0ac5c4015a40ba0e6a940f66e68c18d7aa0680010bd
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:29:56 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:          <none>


Name:               kube-proxy-6tt5l
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se054/10.1.2.54
Start Time:         Fri, 26 Apr 2019 04:35:23 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.54
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://958f986b041043ed05d5bdce8c733d143bc8063f1cf5c93dc44e1b778280ab3b
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:35:24 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:          <none>


Name:               kube-proxy-8c8pw
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se029/10.1.2.29
Start Time:         Fri, 26 Apr 2019 04:30:45 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.29
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://6ff0157e62f4ab1bf3be73e7c343304a9958c367a143586dc9b92b8e3ca9e7ca
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:30:46 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:          <none>


Name:               kube-proxy-8r75f
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se050/10.1.2.50
Start Time:         Fri, 26 Apr 2019 04:34:44 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.50
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://cb30121dcd73b6b2c40234e58d3647325e0c5ff07fa9cb74b59f89487da1448f
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:34:45 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:          <none>


Name:               kube-proxy-96mr4
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se035/10.1.2.35
Start Time:         Fri, 26 Apr 2019 04:32:06 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.35
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://aaa3f7f5c786f9706b0b85169c9dcf95a57eb6b37661aa7a60f020186c80ef55
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:32:07 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:          <none>


Name:               kube-proxy-9h7pt
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se020/10.1.2.20
Start Time:         Fri, 26 Apr 2019 04:28:36 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.20
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://5af255c2fcec757b054ea20d40bc43029286fb369a7835f77ee66ee460eae72c
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:28:37 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type     Reason            Age                     From            Message
  ----     ------            ----                    ----            -------
  Warning  DNSConfigForming  4m38s (x1114 over 23h)  kubelet, se020  Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 147.8.176.15 147.8.178.15 147.8.175.12


Name:               kube-proxy-9hrz7
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se042/10.1.2.42
Start Time:         Fri, 26 Apr 2019 04:33:18 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.42
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://1e3020441b753f4ad524e9f644b6234ab4a9088aafabf45e02f35af2e030f7db
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:33:20 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:          <none>


Name:               kube-proxy-9jfrg
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se016/10.1.2.16
Start Time:         Fri, 26 Apr 2019 04:27:58 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.16
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://d23832b7149b3404cd7da6b3bbb5e5515a3c20b95ff2c86ca0faef33964ff13f
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:28:00 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type     Reason            Age                   From            Message
  ----     ------            ----                  ----            -------
  Warning  DNSConfigForming  40s (x1126 over 23h)  kubelet, se016  Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 147.8.176.15 147.8.178.15 147.8.175.12


Name:               kube-proxy-9nwf7
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se021/10.1.2.21
Start Time:         Fri, 26 Apr 2019 04:28:52 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.21
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://0d33d9ac7722383138a9b4aeea8e75aab9f864b4d09abf37048ffc5c39d883a6
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:28:53 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type     Reason            Age                     From            Message
  ----     ------            ----                    ----            -------
  Warning  DNSConfigForming  4m26s (x1114 over 23h)  kubelet, se021  Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 147.8.176.15 147.8.178.15 147.8.175.12


Name:               kube-proxy-9vhzj
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se014/10.1.2.14
Start Time:         Fri, 26 Apr 2019 04:27:39 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.14
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://62d2e3ff97d0abeb08cb209d0e9971af5c48f60e2a985865f529e9cf34b5b2af
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:27:41 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type     Reason            Age                   From            Message
  ----     ------            ----                  ----            -------
  Warning  DNSConfigForming  52s (x1126 over 23h)  kubelet, se014  Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 147.8.176.15 147.8.178.15 147.8.175.12


Name:               kube-proxy-b7x28
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se040/10.1.2.40
Start Time:         Fri, 26 Apr 2019 04:32:47 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.40
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://3944cbc71594e4ffcdd5d98e55e113b131841447b204e53b121d9ccabacaf0e2
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:32:49 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:          <none>


Name:               kube-proxy-b926g
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se018/10.1.2.18
Start Time:         Fri, 26 Apr 2019 04:28:17 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.18
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://17d5b3ae8f6aa26eb0ad225538596531ac80900f750fdd3aa3e9871eea914463
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:28:18 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type     Reason            Age                   From            Message
  ----     ------            ----                  ----            -------
  Warning  DNSConfigForming  16s (x1122 over 23h)  kubelet, se018  Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 147.8.176.15 147.8.178.15 147.8.175.12


Name:               kube-proxy-bcjtz
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se028/10.1.2.28
Start Time:         Fri, 26 Apr 2019 04:30:29 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.28
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://f1897b58ef07ba1ab4e91a5bcb6765aa6b8d9e40bd3286d05ad197ac86dbfb61
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:30:31 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:          <none>


Name:               kube-proxy-bd89g
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se009/10.1.2.9
Start Time:         Fri, 26 Apr 2019 04:26:02 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.9
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://4082db6f200126600c6785ab1216294dfb113a34cdab8a99976028981fbb3e71
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:26:03 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type     Reason            Age                     From            Message
  ----     ------            ----                    ----            -------
  Warning  DNSConfigForming  2m27s (x1114 over 23h)  kubelet, se009  Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 147.8.176.15 147.8.178.15 147.8.175.12


Name:               kube-proxy-bzbwl
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se046/10.1.2.46
Start Time:         Fri, 26 Apr 2019 04:34:07 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.46
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://5fbe88fc659adcf2e9b974257e7b3fb9ecc33c65ce272e5fc2f33d2dff97fc87
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:34:09 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:          <none>


Name:               kube-proxy-c8n24
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se063/10.1.2.63
Start Time:         Fri, 26 Apr 2019 04:37:19 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.63
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://7220fa7b4f0133139228324972099701e833d1646dc14bd3cd36979a4a97885a
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:37:20 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:          <none>


Name:               kube-proxy-d96l7
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se023/10.1.2.23
Start Time:         Fri, 26 Apr 2019 04:29:24 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.23
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://4e337c75dabcda17a1f9c03ce1c225630b5fa70d9c28f2403aea6c28af5a6b90
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:29:25 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type     Reason            Age                     From            Message
  ----     ------            ----                    ----            -------
  Warning  DNSConfigForming  3m48s (x1114 over 23h)  kubelet, se023  Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 147.8.176.15 147.8.178.15 147.8.175.12


Name:               kube-proxy-d9mzq
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se027/10.1.2.27
Start Time:         Fri, 26 Apr 2019 04:30:26 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.27
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://90756de1466e4904821dbf27789ed3a8739cacadaf0327b440067ce103ba83c8
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:30:27 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:          <none>


Name:               kube-proxy-dh5pl
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se008/10.1.2.8
Start Time:         Fri, 26 Apr 2019 04:25:47 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.8
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://93bb6c1641348f38f51bd5b46862bae752ec4a5427cbf5749b3278026127cf4f
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:25:48 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type     Reason            Age                     From            Message
  ----     ------            ----                    ----            -------
  Warning  DNSConfigForming  2m13s (x1123 over 23h)  kubelet, se008  Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 147.8.176.15 147.8.178.15 147.8.175.12


Name:               kube-proxy-ds4zk
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se058/10.1.2.58
Start Time:         Fri, 26 Apr 2019 04:36:26 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.58
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://0c4b7d751bf5f4b9ece3aa5462a96e7e2d7657fd4bb7229f2f78c20dbdd6ab9d
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:36:27 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:          <none>


Name:               kube-proxy-f69hh
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se055/10.1.2.55
Start Time:         Fri, 26 Apr 2019 04:35:38 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.55
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://48f8573275f0dcc669697e3915e368b1aba8d7a5a0622d2cd7990ace2ac9b1b3
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:35:40 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:          <none>


Name:               kube-proxy-f6t99
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se030/10.1.2.30
Start Time:         Fri, 26 Apr 2019 04:30:48 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.30
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://28c4b2dc477ca9f10105224d2225e5d9fbf86034e881bbe73190be8ee1fef51b
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:30:49 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:          <none>


Name:               kube-proxy-fhq25
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se019/10.1.2.19
Start Time:         Fri, 26 Apr 2019 04:28:34 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.19
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://5f93b0c24f8a70ac32d455587f1250842f731faf361d22c5bea7ae98bbebd8c7
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:28:35 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type     Reason            Age                     From            Message
  ----     ------            ----                    ----            -------
  Warning  DNSConfigForming  3m56s (x1120 over 23h)  kubelet, se019  Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 147.8.176.15 147.8.178.15 147.8.175.12


Name:               kube-proxy-ggjss
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se038/10.1.2.38
Start Time:         Fri, 26 Apr 2019 04:32:29 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.38
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://156eacec0fa7446c837d1f9da3d56289864e3cdba5e900029796f1deb7bb8954
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:32:31 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:          <none>


Name:               kube-proxy-gpk7m
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se045/10.1.2.45
Start Time:         Fri, 26 Apr 2019 04:33:52 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.45
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://e06eb7be204103d6ebcd99d7dbcef429808aabfb924d011a95062a6aef1e7e59
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:33:53 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:          <none>


Name:               kube-proxy-h25n6
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se053/10.1.2.53
Start Time:         Fri, 26 Apr 2019 04:35:19 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.53
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://1deb8ed45aadd7ec13c2fd8a1d332edf7ed0cc876f26157fe41b3bcf1b6202ce
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:35:21 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:          <none>


Name:               kube-proxy-h6pzh
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se004/10.1.2.4
Start Time:         Fri, 26 Apr 2019 04:25:21 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.4
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://d018247f3bc8e14d9c3bba4bf349d85a0b8b7a0f08b2137189a4aa7c8cf33b07
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:25:22 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:          <none>


Name:               kube-proxy-hs9qh
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se057/10.1.2.57
Start Time:         Fri, 26 Apr 2019 04:36:10 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.57
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://388e14c434bb597d7a607fc3386fd3f33e96483d4f87ea09e3d500efaff69961
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:36:11 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:          <none>


Name:               kube-proxy-hznbd
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se007/10.1.2.7
Start Time:         Fri, 26 Apr 2019 04:25:31 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.7
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://e7b3c652617c05976eb89177d09260b2a92f3670281b8183c0dd094f67e5385d
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:25:32 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type     Reason            Age                     From            Message
  ----     ------            ----                    ----            -------
  Warning  DNSConfigForming  2m17s (x1120 over 23h)  kubelet, se007  Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 147.8.176.15 147.8.178.15 147.8.175.12


Name:               kube-proxy-jwrsc
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se022/10.1.2.22
Start Time:         Fri, 26 Apr 2019 04:29:08 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.22
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://3e576d7e6aaf4e06f7ca2bceb03665b42c68460030c37d3e284ea416e97446ae
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:29:09 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type     Reason            Age                     From            Message
  ----     ------            ----                    ----            -------
  Warning  DNSConfigForming  4m16s (x1119 over 23h)  kubelet, se022  Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 147.8.176.15 147.8.178.15 147.8.175.12


Name:               kube-proxy-k4rtp
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se011/10.1.2.11
Start Time:         Fri, 26 Apr 2019 04:27:16 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.11
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://05c7e1a5353a8825aa86b5b52b235817e69c739bb9347eb60954c9a52ec80152
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:27:17 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type     Reason            Age                   From            Message
  ----     ------            ----                  ----            -------
  Warning  DNSConfigForming  64s (x1124 over 23h)  kubelet, se011  Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 147.8.176.15 147.8.178.15 147.8.175.12


Name:               kube-proxy-k4wt7
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se024/10.1.2.24
Start Time:         Fri, 26 Apr 2019 04:29:39 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.24
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://55a85dd2a5f6bb2b7e3779106e05858e3ae67af2e74f753dec3f882d671026cf
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:29:41 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:          <none>


Name:               kube-proxy-kbwcf
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se062/10.1.2.62
Start Time:         Fri, 26 Apr 2019 04:37:04 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.62
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://614b0248e8469cd95f101c36a1c63718f7210823c974ec75f3f79349fd6d22e9
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:37:05 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:          <none>


Name:               kube-proxy-khqv6
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se060/10.1.2.60
Start Time:         Fri, 26 Apr 2019 04:36:44 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.60
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://9250c1d53b7dda4d1546fb7bdd723a3b009653b5a9134993b74b3d86cfa82dd0
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:36:45 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:          <none>


Name:               kube-proxy-l6clr
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se061/10.1.2.61
Start Time:         Fri, 26 Apr 2019 04:36:48 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.61
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://ed970808a0ddcef8e6753a898928d77c4fc02357e45e591ac39b5123ba6f2c0b
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:36:50 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:          <none>


Name:               kube-proxy-lgc6r
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se032/10.1.2.32
Start Time:         Fri, 26 Apr 2019 04:31:20 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.32
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://6a6ac265082af4f8fb96139bc98a9f992cef9b5c09374a938c11c7d5d8dca810
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:31:21 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:          <none>


Name:               kube-proxy-lmtdz
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se059/10.1.2.59
Start Time:         Fri, 26 Apr 2019 04:36:41 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.59
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://926ba69578e15ef4cc2f7a60b49401966f016d402f0800d122d5e48a09e069ad
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:36:42 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:          <none>


Name:               kube-proxy-lr29p
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se017/10.1.2.17
Start Time:         Fri, 26 Apr 2019 04:28:01 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.17
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://a6e77a6df6d1cc2a68b9fc10186b0d458076919d4deddcba9a1dcc31f4b3c3df
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:28:03 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type     Reason            Age                   From            Message
  ----     ------            ----                  ----            -------
  Warning  DNSConfigForming  29s (x1122 over 23h)  kubelet, se017  Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 147.8.176.15 147.8.178.15 147.8.175.12


Name:               kube-proxy-m6bh6
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se064/10.1.2.64
Start Time:         Thu, 25 Apr 2019 12:46:15 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.64
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://35180f1796da6c956c51701001ddac5ded560cf7cb266ef0719d2cbdf80dd4b1
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Thu, 25 Apr 2019 12:46:17 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:          <none>


Name:               kube-proxy-m92ld
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se031/10.1.2.31
Start Time:         Fri, 26 Apr 2019 04:31:04 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.31
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://045ba0fcf7ca29f09c840a06aab489aa4b8fb5f7323cdd3ea4de143cca65c183
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:31:05 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:          <none>


Name:               kube-proxy-mdgvs
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se012/10.1.2.12
Start Time:         Fri, 26 Apr 2019 04:27:20 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.12
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://3bb49f3dc3f618f1b6e3d2eac623af3fe0719c67239e830bcd592b33202b285a
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:27:22 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type     Reason            Age                   From            Message
  ----     ------            ----                  ----            -------
  Warning  DNSConfigForming  23s (x1117 over 23h)  kubelet, se012  Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 147.8.176.15 147.8.178.15 147.8.175.12


Name:               kube-proxy-mnhwz
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se002/10.1.2.2
Start Time:         Sat, 27 Apr 2019 02:34:40 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.2
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://c02533d36301c2d50e867389133941a48e6d42864cd4ba22775cab0416058026
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Sat, 27 Apr 2019 02:34:59 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type     Reason            Age                   From            Message
  ----     ------            ----                  ----            -------
  Warning  DNSConfigForming  3m51s (x64 over 79m)  kubelet, se002  Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 147.8.176.15 147.8.178.15 147.8.175.12


Name:               kube-proxy-mptfq
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se051/10.1.2.51
Start Time:         Fri, 26 Apr 2019 04:34:48 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.51
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://359742ee52ba60a421e9ae1fed27120da05400418ce70e6ae81fabf9bd9b333a
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:34:49 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:          <none>


Name:               kube-proxy-mr9h8
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se048/10.1.2.48
Start Time:         Fri, 26 Apr 2019 04:34:38 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.48
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://857e86fe24b7711d1b9dff899a1632a3fa38263ae7fd387083354bc3a097f33f
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:34:39 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:          <none>


Name:               kube-proxy-msh4s
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se010/10.1.2.10
Start Time:         Fri, 26 Apr 2019 04:27:01 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.10
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://a20b4c3162dd3c537eb137ae39a4c70ddf3beb15927606d7e21bc8d76bb851df
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:27:02 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type     Reason            Age                   From            Message
  ----     ------            ----                  ----            -------
  Warning  DNSConfigForming  93s (x1119 over 23h)  kubelet, se010  Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 147.8.176.15 147.8.178.15 147.8.175.12


Name:               kube-proxy-n4qvf
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se026/10.1.2.26
Start Time:         Fri, 26 Apr 2019 04:30:10 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.26
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://1310984d55c01f05e19283dd9f2107b491e06be8e164883e02b5f5be36e3863e
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:30:11 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:          <none>


Name:               kube-proxy-njrzw
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se033/10.1.2.33
Start Time:         Fri, 26 Apr 2019 04:31:36 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.33
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://dd1697136cf38c02ef2d075c6bb4835eb5a5aaec753a8724e3e00803d78238bf
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:31:37 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:          <none>


Name:               kube-proxy-nvlg9
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se015/10.1.2.15
Start Time:         Fri, 26 Apr 2019 04:27:55 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.15
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://213a70af2f2a0ec7f6e7e35506428a5ae77a76659e9809e774950c56203d4e35
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:27:56 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type     Reason            Age                   From            Message
  ----     ------            ----                  ----            -------
  Warning  DNSConfigForming  44s (x1117 over 23h)  kubelet, se015  Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 147.8.176.15 147.8.178.15 147.8.175.12


Name:               kube-proxy-qbl2x
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se052/10.1.2.52
Start Time:         Fri, 26 Apr 2019 04:35:04 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.52
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://dc7066185f3b6cb79f21e57dbf567f6483759063a1fbf354bb2a17a5cd932876
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:35:05 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:          <none>


Name:               kube-proxy-qmfgs
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se013/10.1.2.13
Start Time:         Fri, 26 Apr 2019 04:27:24 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.13
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://4618f96b43defd937ee03707a42e4e842e1d243e6fce24edc9595fb6dbd68cb4
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:27:25 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type     Reason            Age                   From            Message
  ----     ------            ----                  ----            -------
  Warning  DNSConfigForming  81s (x1111 over 23h)  kubelet, se013  Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 147.8.176.15 147.8.178.15 147.8.175.12


Name:               kube-proxy-rklq9
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se056/10.1.2.56
Start Time:         Fri, 26 Apr 2019 04:35:55 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.56
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://2924a427239963dea1fa1fe81611b196dcf2a319ef5cfb303e112373ca588f05
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:35:57 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:          <none>


Name:               kube-proxy-rs56r
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se003/10.1.2.3
Start Time:         Fri, 26 Apr 2019 04:25:05 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.3
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://7be39d0d492a11ae0966e830645d0bf4187588d1b9208bc59cd0703f2e0f841e
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:25:06 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type     Reason            Age                     From            Message
  ----     ------            ----                    ----            -------
  Warning  DNSConfigForming  2m45s (x1118 over 23h)  kubelet, se003  Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 147.8.176.15 147.8.178.15 147.8.175.12


Name:               kube-proxy-rxrpz
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se047/10.1.2.47
Start Time:         Fri, 26 Apr 2019 04:34:22 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.47
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://03f921f66caa598dde25ecf3ce2f18459856597f8fae859ce36f11d1fd977fc3
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:34:24 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:          <none>


Name:               kube-proxy-t7xsj
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se001/10.1.2.1
Start Time:         Fri, 26 Apr 2019 04:22:33 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.1
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://6cebaeb9548744635ec6dc9134d0fdaa453ad40ede3060976916b6507cf2d98f
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:22:34 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type     Reason            Age                   From            Message
  ----     ------            ----                  ----            -------
  Warning  DNSConfigForming  50s (x1128 over 23h)  kubelet, se001  Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 147.8.176.15 147.8.178.15 147.8.175.12


Name:               kube-proxy-tsnmp
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se034/10.1.2.34
Start Time:         Fri, 26 Apr 2019 04:31:51 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.34
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://cf7ad19913bb8584fd8919cf1f983d559caa58bf66b949ab4c0ee97e5eb8698c
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:31:52 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:          <none>


Name:               kube-proxy-v44d6
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se037/10.1.2.37
Start Time:         Fri, 26 Apr 2019 04:32:13 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.37
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://9aa113b878c7352bff9e7efa728a567c1367280b5b496636b5fa27a1a068efdc
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:32:15 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:          <none>


Name:               kube-proxy-w6fm7
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se049/10.1.2.49
Start Time:         Fri, 26 Apr 2019 04:34:41 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.49
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://64bed9168dc2c7cff4bc663df287bb527198bb7f503d054d706b82515b5e9cf7
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:34:42 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:          <none>


Name:               kube-proxy-wkqh5
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se005/10.1.2.5
Start Time:         Fri, 26 Apr 2019 04:25:24 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.5
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://d1b493dad99c4a1064d0c7c69f5259f5384f2eeb34787229889d64a8142d1985
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:25:25 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:
  Type     Reason            Age                    From            Message
  ----     ------            ----                   ----            -------
  Warning  DNSConfigForming  2m9s (x1121 over 23h)  kubelet, se005  Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 147.8.176.15 147.8.178.15 147.8.175.12


Name:               kube-proxy-wzssh
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se043/10.1.2.43
Start Time:         Fri, 26 Apr 2019 04:33:33 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.43
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://1d535b624074092d36213608854c01a075067013b7adb19d72aeacd1b1b6e8fb
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:33:35 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:          <none>


Name:               kube-proxy-zsr8r
Namespace:          kube-system
Priority:           2000001000
PriorityClassName:  system-node-critical
Node:               se036/10.1.2.36
Start Time:         Fri, 26 Apr 2019 04:32:09 +0000
Labels:             controller-revision-hash=6488cfdd59
                    k8s-app=kube-proxy
                    pod-template-generation=1
Annotations:        <none>
Status:             Running
IP:                 10.1.2.36
Controlled By:      DaemonSet/kube-proxy
Containers:
  kube-proxy:
    Container ID:  docker://57226cff33fddf376346a4fbcb524a788e762341b0041de5f62c74743a931562
    Image:         k8s.gcr.io/kube-proxy:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-proxy@sha256:44af2833c6cbd9a7fc2e9d2f5244a39dfd2e31ad91bf9d4b7d810678db738ee9
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/local/bin/kube-proxy
      --config=/var/lib/kube-proxy/config.conf
      --hostname-override=$(NODE_NAME)
    State:          Running
      Started:      Fri, 26 Apr 2019 04:32:11 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      NODE_NAME:   (v1:spec.nodeName)
    Mounts:
      /lib/modules from lib-modules (ro)
      /run/xtables.lock from xtables-lock (rw)
      /var/lib/kube-proxy from kube-proxy (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-proxy-token-t2mgn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-proxy:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kube-proxy
    Optional:  false
  xtables-lock:
    Type:          HostPath (bare host directory volume)
    Path:          /run/xtables.lock
    HostPathType:  FileOrCreate
  lib-modules:
    Type:          HostPath (bare host directory volume)
    Path:          /lib/modules
    HostPathType:  
  kube-proxy-token-t2mgn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kube-proxy-token-t2mgn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     
                 CriticalAddonsOnly
                 node.kubernetes.io/disk-pressure:NoSchedule
                 node.kubernetes.io/memory-pressure:NoSchedule
                 node.kubernetes.io/network-unavailable:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute
                 node.kubernetes.io/pid-pressure:NoSchedule
                 node.kubernetes.io/unreachable:NoExecute
                 node.kubernetes.io/unschedulable:NoSchedule
Events:          <none>


Name:               kube-scheduler-se064
Namespace:          kube-system
Priority:           2000000000
PriorityClassName:  system-cluster-critical
Node:               se064/10.1.2.64
Start Time:         Thu, 25 Apr 2019 12:45:48 +0000
Labels:             component=kube-scheduler
                    tier=control-plane
Annotations:        kubernetes.io/config.hash: f44110a0ca540009109bfc32a7eb0baa
                    kubernetes.io/config.mirror: f44110a0ca540009109bfc32a7eb0baa
                    kubernetes.io/config.seen: 2019-04-25T12:45:47.648698054Z
                    kubernetes.io/config.source: file
Status:             Running
IP:                 10.1.2.64
Containers:
  kube-scheduler:
    Container ID:  docker://1d47f1e38b540ae914a1cb6948c35832cc9d8c5941f64a56c0c9c35d45d41618
    Image:         k8s.gcr.io/kube-scheduler:v1.14.1
    Image ID:      docker-pullable://k8s.gcr.io/kube-scheduler@sha256:11af0ae34bc63cdc78b8bd3256dff1ba96bf2eee4849912047dee3e420b52f8f
    Port:          <none>
    Host Port:     <none>
    Command:
      kube-scheduler
      --bind-address=127.0.0.1
      --kubeconfig=/etc/kubernetes/scheduler.conf
      --leader-elect=true
    State:          Running
      Started:      Thu, 25 Apr 2019 12:45:50 +0000
    Ready:          True
    Restart Count:  0
    Requests:
      cpu:        100m
    Liveness:     http-get http://127.0.0.1:10251/healthz delay=15s timeout=15s period=10s #success=1 #failure=8
    Environment:  <none>
    Mounts:
      /etc/kubernetes/scheduler.conf from kubeconfig (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kubeconfig:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/kubernetes/scheduler.conf
    HostPathType:  FileOrCreate
QoS Class:         Burstable
Node-Selectors:    <none>
Tolerations:       :NoExecute
Events:            <none>


Name:               kubernetes-dashboard-5fd74ddbcd-xtlh9
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se011/10.1.2.11
Start Time:         Sat, 27 Apr 2019 03:48:53 +0000
Labels:             k8s-app=kubernetes-dashboard
                    pod-template-hash=5fd74ddbcd
Annotations:        <none>
Status:             Pending
IP:                 
Controlled By:      ReplicaSet/kubernetes-dashboard-5fd74ddbcd
Containers:
  kubernetes-dashboard:
    Container ID:  
    Image:         k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1
    Image ID:      
    Port:          8443/TCP
    Host Port:     0/TCP
    Args:
      --auto-generate-certificates
      --heapster-host=http://heapster.kube-system:80
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Liveness:       http-get https://:8443/ delay=30s timeout=30s period=10s #success=1 #failure=3
    Environment:    <none>
    Mounts:
      /certs from kubernetes-dashboard-certs (rw)
      /tmp from tmp-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kubernetes-dashboard-token-whhkn (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  kubernetes-dashboard-certs:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kubernetes-dashboard-certs
    Optional:    false
  tmp-volume:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:     
    SizeLimit:  <unset>
  kubernetes-dashboard-token-whhkn:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  kubernetes-dashboard-token-whhkn
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node-role.kubernetes.io/master:NoSchedule
                 node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  4m53s  default-scheduler  Successfully assigned kube-system/kubernetes-dashboard-5fd74ddbcd-xtlh9 to se011
  Normal  Pulling    4m51s  kubelet, se011     Pulling image "k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.1"


Name:               monitoring-grafana-cd55c5947-2kkvt
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se049/10.1.2.49
Start Time:         Fri, 26 Apr 2019 04:50:43 +0000
Labels:             k8s-app=grafana
                    pod-template-hash=cd55c5947
                    task=monitoring
Annotations:        <none>
Status:             Running
IP:                 10.244.135.2
Controlled By:      ReplicaSet/monitoring-grafana-cd55c5947
Containers:
  grafana:
    Container ID:   docker://d5e630189ffcbdc81671247455e5f163d843e8085880503b9781e126cb7bb1dc
    Image:          k8s.gcr.io/heapster-grafana-amd64:v5.0.4
    Image ID:       docker-pullable://k8s.gcr.io/heapster-grafana-amd64@sha256:720cfe4a35e1065dbde42d2312fb827df200ced867353e3afe497c81c8aa4a56
    Port:           3000/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Fri, 26 Apr 2019 04:50:57 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      INFLUXDB_HOST:               monitoring-influxdb
      GF_SERVER_HTTP_PORT:         3000
      GF_AUTH_BASIC_ENABLED:       false
      GF_AUTH_ANONYMOUS_ENABLED:   true
      GF_AUTH_ANONYMOUS_ORG_ROLE:  Admin
      GF_SERVER_ROOT_URL:          /
    Mounts:
      /etc/ssl/certs from ca-certificates (ro)
      /var from grafana-storage (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-ds9br (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  ca-certificates:
    Type:          HostPath (bare host directory volume)
    Path:          /etc/ssl/certs
    HostPathType:  
  grafana-storage:
    Type:          HostPath (bare host directory volume)
    Path:          /localfs/xyao/k8s/grafana/data
    HostPathType:  
  default-token-ds9br:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-ds9br
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>


Name:               monitoring-influxdb-6c9b99889d-mhjcv
Namespace:          kube-system
Priority:           0
PriorityClassName:  <none>
Node:               se050/10.1.2.50
Start Time:         Fri, 26 Apr 2019 04:50:51 +0000
Labels:             k8s-app=influxdb
                    pod-template-hash=6c9b99889d
                    task=monitoring
Annotations:        <none>
Status:             Running
IP:                 10.244.136.2
Controlled By:      ReplicaSet/monitoring-influxdb-6c9b99889d
Containers:
  influxdb:
    Container ID:   docker://7fe35b184443721ece617ce659ce336c5f9774d515276b30821d48806e523bae
    Image:          k8s.gcr.io/heapster-influxdb-amd64:v1.5.2
    Image ID:       docker-pullable://k8s.gcr.io/heapster-influxdb-amd64@sha256:06f3919887956ce5f5a693b96e7c54c80f54253f353fed8234da3d51dddef7e1
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Fri, 26 Apr 2019 04:50:55 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /data from influxdb-storage (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-ds9br (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  influxdb-storage:
    Type:          HostPath (bare host directory volume)
    Path:          /localfs/xyao/k8s/influxdb/data
    HostPathType:  
  default-token-ds9br:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-ds9br
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:          <none>
